{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dxcam_cpp as dxcam\n",
    "from src.utils.windowtools import (\n",
    "    fuzzy_window_search,\n",
    "    calculate_aspect_ratio,\n",
    "    check_aspect_ratio_validity,\n",
    "    get_monitor_number_from_coords,\n",
    "    normalise_coords_to_monitor\n",
    ")\n",
    "from src.utils.helpers import (\n",
    "    pre_process,\n",
    "    pre_process_distbox,\n",
    ")\n",
    "from src.models import get_model, get_default_model_type, get_model_info\n",
    "import matplotlib.pyplot as plt\n",
    "from easyocr import Reader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tkinter as tk\n",
    "import threading\n",
    "import time as systime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = fuzzy_window_search(\"asphalt\")\n",
    "\n",
    "monitor_id = get_monitor_number_from_coords(coords)\n",
    "\n",
    "normalised_coords = normalise_coords_to_monitor(coords, monitor_id)\n",
    "\n",
    "aspect_ratio = calculate_aspect_ratio(normalised_coords)\n",
    "\n",
    "check_aspect_ratio_validity(aspect_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DIGIT RECOGNITION USING TEMPLATE MATCHING ===\n",
    "\n",
    "# Configuration for template matching\n",
    "TEMPLATE_DIR = \"timer_templates\"  # Directory containing digit templates\n",
    "MATCH_THRESHOLD = 0.7  # Confidence threshold for template matching\n",
    "ITALIC_SHEAR_ANGLE = -15  # Degrees to correct italic text\n",
    "\n",
    "def load_digit_templates():\n",
    "    \"\"\"\n",
    "    Load the manually created digit templates (0-9) from the processed directory.\n",
    "    \"\"\"\n",
    "    templates = {}\n",
    "    \n",
    "    if not os.path.exists(TEMPLATE_DIR):\n",
    "        print(f\"Template directory {TEMPLATE_DIR} not found!\")\n",
    "        return templates\n",
    "    \n",
    "    # Load digit templates (0-9)\n",
    "    for digit in range(10):\n",
    "        template_path = os.path.join(TEMPLATE_DIR, f\"{digit}.png\")\n",
    "        \n",
    "        if os.path.exists(template_path):\n",
    "            template = cv2.imread(template_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if template is not None:\n",
    "                templates[str(digit)] = template\n",
    "                print(f\"Loaded template for digit '{digit}' (size: {template.shape[1]}x{template.shape[0]})\")\n",
    "            else:\n",
    "                print(f\"Failed to load template: {template_path}\")\n",
    "        else:\n",
    "            print(f\"Template not found: {template_path}\")\n",
    "    \n",
    "    return templates\n",
    "\n",
    "def correct_italic_text(image, shear_angle_degrees=ITALIC_SHEAR_ANGLE):\n",
    "    \"\"\"\n",
    "    Correct italic text by applying inverse shear transformation.\n",
    "    \"\"\"\n",
    "    height, width = image.shape\n",
    "    \n",
    "    # Convert angle to radians\n",
    "    shear_angle = np.radians(shear_angle_degrees)\n",
    "    \n",
    "    # Calculate shear transformation matrix\n",
    "    shear_factor = -np.tan(shear_angle)  # Negative for correction\n",
    "    \n",
    "    # Create affine transformation matrix\n",
    "    M = np.float32([[1, shear_factor, 0], [0, 1, 0]])\n",
    "    \n",
    "    # Calculate new width after shearing\n",
    "    new_width = int(width + abs(shear_factor * height))\n",
    "    \n",
    "    # Apply shear transformation with white padding\n",
    "    corrected = cv2.warpAffine(image, M, (new_width, height), \n",
    "                              borderMode=cv2.BORDER_CONSTANT, \n",
    "                              borderValue=255)  # White padding\n",
    "    \n",
    "    return corrected\n",
    "\n",
    "def preprocess_timer_image(image):\n",
    "    \"\"\"\n",
    "    Preprocess timer image: correct italics, enhance contrast, ensure binary.\n",
    "    \"\"\"\n",
    "    # Correct italic text\n",
    "    corrected = correct_italic_text(image)\n",
    "    \n",
    "    # Enhance contrast\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(corrected)\n",
    "    \n",
    "    # Reduce noise\n",
    "    denoised = cv2.medianBlur(enhanced, 3)\n",
    "    \n",
    "    # Ensure binary (white background, black text)\n",
    "    _, binary = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def match_digit_at_position(roi_image, templates, threshold=MATCH_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Match a character ROI against digit templates (0-9).\n",
    "    Returns the best matching digit and confidence.\n",
    "    \"\"\"\n",
    "    best_digit = None\n",
    "    best_confidence = 0\n",
    "    \n",
    "    # Ensure ROI is grayscale\n",
    "    if len(roi_image.shape) == 3:\n",
    "        roi_image = cv2.cvtColor(roi_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Ensure binary\n",
    "    _, roi_binary = cv2.threshold(roi_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Try multiple scale factors for robust matching\n",
    "    scale_factors = [0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3]\n",
    "    \n",
    "    for digit, template in templates.items():\n",
    "        max_confidence_for_digit = 0\n",
    "        \n",
    "        for scale_factor in scale_factors:\n",
    "            # Scale the template to match ROI\n",
    "            scaled_height = int(template.shape[0] * scale_factor)\n",
    "            scaled_width = int(template.shape[1] * scale_factor)\n",
    "            \n",
    "            if scaled_height > 0 and scaled_width > 0:\n",
    "                template_resized = cv2.resize(template, (scaled_width, scaled_height), \n",
    "                                            interpolation=cv2.INTER_CUBIC)\n",
    "                \n",
    "                # Template matching using normalized cross correlation\n",
    "                if (roi_binary.shape[0] >= template_resized.shape[0] and \n",
    "                    roi_binary.shape[1] >= template_resized.shape[1]):\n",
    "                    result = cv2.matchTemplate(roi_binary, template_resized, cv2.TM_CCOEFF_NORMED)\n",
    "                    confidence = np.max(result)\n",
    "                elif (template_resized.shape[0] >= roi_binary.shape[0] and \n",
    "                      template_resized.shape[1] >= roi_binary.shape[1]):\n",
    "                    # If template is larger, match ROI against template\n",
    "                    result = cv2.matchTemplate(template_resized, roi_binary, cv2.TM_CCOEFF_NORMED)\n",
    "                    confidence = np.max(result)\n",
    "                else:\n",
    "                    confidence = 0\n",
    "                \n",
    "                max_confidence_for_digit = max(max_confidence_for_digit, confidence)\n",
    "        \n",
    "        if max_confidence_for_digit > best_confidence:\n",
    "            best_confidence = max_confidence_for_digit\n",
    "            best_digit = digit\n",
    "    \n",
    "    if best_confidence >= threshold:\n",
    "        return best_digit, best_confidence\n",
    "    else:\n",
    "        return None, best_confidence\n",
    "\n",
    "def find_digit_regions(processed_image):\n",
    "    \"\"\"\n",
    "    Find potential digit regions in the processed image using contour detection.\n",
    "    Returns a list of (x, y, w, h) bounding boxes sorted left to right.\n",
    "    \"\"\"\n",
    "    # Invert image so text is white on black background for contour detection\n",
    "    inverted = cv2.bitwise_not(processed_image)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(inverted, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Filter contours by size and aspect ratio\n",
    "    digit_regions = []\n",
    "    \n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        area = cv2.contourArea(contour)\n",
    "        \n",
    "        # Filter based on reasonable digit dimensions\n",
    "        if (w >= 8 and h >= 12 and w <= 100 and h <= 100 and  # Size constraints\n",
    "            area > 50 and  # Minimum area\n",
    "            h/w >= 0.8 and h/w <= 4.0):  # Aspect ratio for digits\n",
    "            digit_regions.append((x, y, w, h))\n",
    "    \n",
    "    # Sort regions from left to right\n",
    "    digit_regions.sort(key=lambda region: region[0])\n",
    "    \n",
    "    return digit_regions\n",
    "\n",
    "def extract_digits_from_timer(image, templates, debug=False):\n",
    "    \"\"\"\n",
    "    Extract only digits (0-9) from a timer image, ignoring punctuation.\n",
    "    Returns the digits string and total milliseconds.\n",
    "    \"\"\"\n",
    "    # Preprocess the image\n",
    "    processed_image = preprocess_timer_image(image)\n",
    "    \n",
    "    # Find potential digit regions\n",
    "    digit_regions = find_digit_regions(processed_image)\n",
    "    \n",
    "    recognized_digits = []\n",
    "    digit_details = []\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Found {len(digit_regions)} potential digit regions\")\n",
    "    \n",
    "    for i, (x, y, w, h) in enumerate(digit_regions):\n",
    "        # Add some padding around the region\n",
    "        padding = max(2, min(w, h) // 8)\n",
    "        x_start = max(0, x - padding)\n",
    "        y_start = max(0, y - padding)\n",
    "        x_end = min(processed_image.shape[1], x + w + padding)\n",
    "        y_end = min(processed_image.shape[0], y + h + padding)\n",
    "        \n",
    "        # Extract the digit region\n",
    "        digit_roi = processed_image[y_start:y_end, x_start:x_end]\n",
    "        \n",
    "        if digit_roi.size > 0:\n",
    "            # Match against digit templates\n",
    "            digit, confidence = match_digit_at_position(digit_roi, templates)\n",
    "            \n",
    "            if digit is not None:\n",
    "                recognized_digits.append(digit)\n",
    "                digit_details.append((digit, confidence, (x, y, w, h), digit_roi))\n",
    "                if debug:\n",
    "                    print(f\"  Region {i}: Digit '{digit}' (confidence: {confidence:.3f})\")\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"  Region {i}: No match (best confidence: {confidence:.3f})\")\n",
    "    \n",
    "    # Join all recognized digits into a string\n",
    "    digits_only = ''.join(recognized_digits)\n",
    "    \n",
    "    return digits_only, digit_details, processed_image\n",
    "\n",
    "def convert_timer_to_milliseconds(timer_string):\n",
    "    \"\"\"\n",
    "    Convert timer string in format mmssxxx to total milliseconds.\n",
    "    mm = minutes (2 digits)\n",
    "    ss = seconds (2 digits) \n",
    "    xxx = milliseconds (3 digits)\n",
    "    \"\"\"\n",
    "    if not timer_string or len(timer_string) < 7:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Extract components (expecting at least 7 digits: mmssxxx)\n",
    "        if len(timer_string) >= 7:\n",
    "            # Take first 7 digits to handle mmssxxx format\n",
    "            timer_digits = timer_string[:7]\n",
    "            \n",
    "            minutes = int(timer_digits[0:2])\n",
    "            seconds = int(timer_digits[2:4])\n",
    "            milliseconds = int(timer_digits[4:7])\n",
    "            \n",
    "            # Convert to total milliseconds\n",
    "            total_ms = (minutes * 60 * 1000) + (seconds * 1000) + milliseconds\n",
    "            \n",
    "            return total_ms\n",
    "        else:\n",
    "            return None\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "# Load digit templates at startup\n",
    "print(\"Loading digit templates for timer recognition...\")\n",
    "digit_templates = load_digit_templates()\n",
    "print(f\"Loaded {len(digit_templates)} digit templates for timer recognition\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars\n",
    "camera = dxcam.create(device_idx=0, output_idx=monitor_id)\n",
    "capturing = True\n",
    "time = 0\n",
    "elapsed_ms = 0\n",
    "percentage = 0\n",
    "race_in_progress = False  # New variable to track race state\n",
    "\n",
    "# Inference time tracking\n",
    "inference_times = []\n",
    "total_loops = 0\n",
    "avg_inference_time = 0\n",
    "\n",
    "# Loop time tracking\n",
    "loop_times = []\n",
    "avg_loop_time = 0\n",
    "\n",
    "# Timer tracking variables\n",
    "current_timer_ms = 0  # Current timer in total milliseconds\n",
    "current_timer_display = \"00:00.000\"  # Formatted timer display\n",
    "\n",
    "reader = Reader(['en'], gpu=True)  # Still needed for DIST detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a frame from the camera\n",
    "window = camera.grab()\n",
    "\n",
    "# Extract coordinates from the coords variable\n",
    "x1, y1, x2, y2 = normalised_coords\n",
    "\n",
    "capture_coords = (x1, y1, x2, int(y1 + (y2 - y1) / 3.4))\n",
    "\n",
    "camera.start(region=capture_coords, target_fps=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_capturing():\n",
    "    global capturing\n",
    "    capturing = True\n",
    "\n",
    "def stop_capturing():\n",
    "    global capturing\n",
    "    capturing = False\n",
    "\n",
    "def close_application():\n",
    "    \"\"\"Close the application and stop the script\"\"\"\n",
    "    global capturing, root\n",
    "    capturing = False\n",
    "    if root:\n",
    "        root.quit()\n",
    "        root.destroy()\n",
    "    import sys\n",
    "    sys.exit(0)\n",
    "\n",
    "def toggle_pin():\n",
    "    global root, is_pinned\n",
    "    is_pinned = not is_pinned\n",
    "    if is_pinned:\n",
    "        root.wm_attributes(\"-topmost\", True)\n",
    "        pin_button.config(text=\"ðŸ“Œ\", bg=\"#4a4a4a\")\n",
    "    else:\n",
    "        root.wm_attributes(\"-topmost\", False)\n",
    "        pin_button.config(text=\"ðŸ“Œ\", bg=\"#4a4a4a\")\n",
    "\n",
    "def toggle_debug_panel():\n",
    "    \"\"\"Toggle the debug panel visibility\"\"\"\n",
    "    global debug_expanded, debug_frame, expand_button\n",
    "    debug_expanded = not debug_expanded\n",
    "    \n",
    "    if debug_expanded:\n",
    "        debug_frame.pack(fill=\"both\", expand=True, padx=5, pady=(0, 5))\n",
    "        expand_button.config(text=\"âˆ’\", bg=\"#2196F3\")\n",
    "        root.geometry(\"400x280\")  # Expanded height\n",
    "    else:\n",
    "        debug_frame.pack_forget()\n",
    "        expand_button.config(text=\"+\", bg=\"#2196F3\")\n",
    "        root.geometry(\"400x120\")  # Compact height\n",
    "\n",
    "def on_drag_start(event):\n",
    "    \"\"\"Start window dragging\"\"\"\n",
    "    global drag_start_x, drag_start_y\n",
    "    drag_start_x = event.x\n",
    "    drag_start_y = event.y\n",
    "\n",
    "def on_drag_motion(event):\n",
    "    \"\"\"Handle window dragging\"\"\"\n",
    "    global drag_start_x, drag_start_y\n",
    "    x = root.winfo_x() + (event.x - drag_start_x)\n",
    "    y = root.winfo_y() + (event.y - drag_start_y)\n",
    "    root.geometry(f\"+{x}+{y}\")\n",
    "\n",
    "def update_ui():\n",
    "    global time, elapsed_ms, percentage, avg_inference_time, inference_times, race_in_progress\n",
    "    global current_timer_ms, current_timer_display, avg_loop_time, delta_display\n",
    "    \n",
    "    # Update the main delta display (placeholder for now)\n",
    "    delta_label.config(text=delta_display)\n",
    "    \n",
    "    # Update debug info only if expanded\n",
    "    if debug_expanded:\n",
    "        time_label.config(text=f\"Timer: {current_timer_display}\")\n",
    "        elapsed_label.config(text=f\"Loop: {elapsed_ms:.1f}ms\")\n",
    "        avg_loop_label.config(text=f\"Avg Loop: {avg_loop_time:.1f}ms\")\n",
    "        \n",
    "        # Smart status display based on race_in_progress\n",
    "        if race_in_progress and percentage and percentage != \"0%\":\n",
    "            try:\n",
    "                progress_value = int(percentage.replace('%', ''))\n",
    "                progress_bar.config(value=progress_value)\n",
    "                progress_label.config(text=f\"{progress_value}%\")\n",
    "            except:\n",
    "                progress_value = 0\n",
    "                progress_bar.config(value=0)\n",
    "                progress_label.config(text=\"0%\")\n",
    "            \n",
    "            percentage_label.config(text=f\"Distance: {percentage}\", fg=\"#2ecc71\")\n",
    "            status_label.config(text=\"Race In Progress\", fg=\"#2ecc71\")\n",
    "        else:\n",
    "            progress_bar.config(value=0)\n",
    "            progress_label.config(text=\"--\")\n",
    "            percentage_label.config(text=\"Distance: --\", fg=\"#95a5a6\")\n",
    "            if race_in_progress:\n",
    "                status_label.config(text=\"Searching For Race...\", fg=\"#f39c12\")\n",
    "            else:\n",
    "                status_label.config(text=\"Race Not Started\", fg=\"#95a5a6\")\n",
    "        \n",
    "        # Performance metrics\n",
    "        if inference_times:\n",
    "            current_inference = inference_times[-1] if inference_times else 0\n",
    "            inference_label.config(text=f\"Inference: {current_inference:.1f}ms\")\n",
    "            avg_inference_label.config(text=f\"Average: {avg_inference_time:.1f}ms\")\n",
    "        else:\n",
    "            inference_label.config(text=\"Inference: --\")\n",
    "            avg_inference_label.config(text=\"Average: --\")\n",
    "\n",
    "    # Schedule next update at 11ms (90 FPS) for ultra-responsive UI\n",
    "    root.after(11, update_ui)\n",
    "\n",
    "def create_ui():\n",
    "    global root, time_label, elapsed_label, percentage_label, status_label\n",
    "    global avg_inference_label, inference_label, pin_button, is_pinned\n",
    "    global progress_bar, progress_label, current_timer_display, avg_loop_label\n",
    "    global debug_frame, debug_expanded, expand_button, delta_label, delta_display\n",
    "    global drag_start_x, drag_start_y\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.title(\"ALU Timing Tool\")\n",
    "    root.geometry(\"400x120\")  # Compact initial size\n",
    "    root.resizable(False, False)\n",
    "    \n",
    "    # Remove window decorations (borderless)\n",
    "    root.overrideredirect(True)\n",
    "    \n",
    "    # Set up the window style - dark theme\n",
    "    root.configure(bg=\"#1e1e1e\")\n",
    "    is_pinned = False\n",
    "    debug_expanded = False\n",
    "    delta_display = \"+99.999\"  # Placeholder delta\n",
    "    \n",
    "    # Drag variables\n",
    "    drag_start_x = 0\n",
    "    drag_start_y = 0\n",
    "    \n",
    "    # Main container that can be dragged\n",
    "    main_frame = tk.Frame(root, bg=\"#1e1e1e\")\n",
    "    main_frame.pack(fill=\"both\", expand=True)\n",
    "    main_frame.bind(\"<Button-1>\", on_drag_start)\n",
    "    main_frame.bind(\"<B1-Motion>\", on_drag_motion)\n",
    "    \n",
    "    # Top control bar with buttons\n",
    "    control_frame = tk.Frame(main_frame, bg=\"#1e1e1e\", height=25)\n",
    "    control_frame.pack(fill=\"x\", padx=5, pady=5)\n",
    "    control_frame.pack_propagate(False)\n",
    "    \n",
    "    # Pin button (top right area)\n",
    "    pin_button = tk.Button(control_frame, text=\"ðŸ“Œ\", command=toggle_pin,\n",
    "                          bg=\"#4a4a4a\", fg=\"white\", font=(\"Segoe UI\", 10),\n",
    "                          relief=\"flat\", width=3, height=1, bd=0)\n",
    "    pin_button.pack(side=\"right\", padx=(2, 0))\n",
    "    \n",
    "    # Close button (top right corner)\n",
    "    close_button = tk.Button(control_frame, text=\"âœ•\", command=close_application,\n",
    "                            bg=\"#e74c3c\", fg=\"white\", font=(\"Segoe UI\", 10, \"bold\"),\n",
    "                            relief=\"flat\", width=3, height=1, bd=0)\n",
    "    close_button.pack(side=\"right\")\n",
    "    \n",
    "    # Main delta display area\n",
    "    delta_frame = tk.Frame(main_frame, bg=\"#1e1e1e\")\n",
    "    delta_frame.pack(fill=\"both\", expand=True, padx=5, pady=(0, 5))\n",
    "    delta_frame.bind(\"<Button-1>\", on_drag_start)\n",
    "    delta_frame.bind(\"<B1-Motion>\", on_drag_motion)\n",
    "    \n",
    "    # Large delta display\n",
    "    delta_label = tk.Label(delta_frame, text=delta_display,\n",
    "                          font=(\"Segoe UI\", 36, \"bold\"), fg=\"#00ff00\", bg=\"#1e1e1e\")\n",
    "    delta_label.pack(expand=True)\n",
    "    delta_label.bind(\"<Button-1>\", on_drag_start)\n",
    "    delta_label.bind(\"<B1-Motion>\", on_drag_motion)\n",
    "    \n",
    "    # Bottom control bar\n",
    "    bottom_frame = tk.Frame(main_frame, bg=\"#1e1e1e\", height=25)\n",
    "    bottom_frame.pack(fill=\"x\", padx=5, pady=(0, 5))\n",
    "    bottom_frame.pack_propagate(False)\n",
    "    bottom_frame.bind(\"<Button-1>\", on_drag_start)\n",
    "    bottom_frame.bind(\"<B1-Motion>\", on_drag_motion)\n",
    "    \n",
    "    # Expand/collapse button (bottom right)\n",
    "    expand_button = tk.Button(bottom_frame, text=\"+\", command=toggle_debug_panel,\n",
    "                             bg=\"#2196F3\", fg=\"white\", font=(\"Segoe UI\", 12, \"bold\"),\n",
    "                             relief=\"flat\", width=3, height=1, bd=0)\n",
    "    expand_button.pack(side=\"right\")\n",
    "    \n",
    "    # Debug panel (initially hidden)\n",
    "    debug_frame = tk.Frame(main_frame, bg=\"#2a2a2a\", relief=\"solid\", bd=1)\n",
    "    \n",
    "    # Debug content\n",
    "    debug_title = tk.Label(debug_frame, text=\"Debug Information\",\n",
    "                          font=(\"Segoe UI\", 10, \"bold\"), fg=\"#ffffff\", bg=\"#2a2a2a\")\n",
    "    debug_title.pack(pady=5)\n",
    "    \n",
    "    # Performance metrics in debug panel\n",
    "    perf_frame = tk.Frame(debug_frame, bg=\"#2a2a2a\")\n",
    "    perf_frame.pack(fill=\"x\", padx=10, pady=5)\n",
    "    \n",
    "    # Left column\n",
    "    left_debug = tk.Frame(perf_frame, bg=\"#2a2a2a\")\n",
    "    left_debug.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "    \n",
    "    time_label = tk.Label(left_debug, text=\"Timer: --\",\n",
    "                         font=(\"Segoe UI\", 9), fg=\"#ffffff\", bg=\"#2a2a2a\")\n",
    "    time_label.pack(anchor=\"w\")\n",
    "    \n",
    "    elapsed_label = tk.Label(left_debug, text=\"Loop: --\",\n",
    "                            font=(\"Segoe UI\", 9), fg=\"#ffffff\", bg=\"#2a2a2a\")\n",
    "    elapsed_label.pack(anchor=\"w\")\n",
    "    \n",
    "    avg_loop_label = tk.Label(left_debug, text=\"Avg Loop: --\",\n",
    "                             font=(\"Segoe UI\", 9), fg=\"#ffffff\", bg=\"#2a2a2a\")\n",
    "    avg_loop_label.pack(anchor=\"w\")\n",
    "    \n",
    "    # Right column\n",
    "    right_debug = tk.Frame(perf_frame, bg=\"#2a2a2a\")\n",
    "    right_debug.pack(side=\"right\", fill=\"both\", expand=True)\n",
    "    \n",
    "    inference_label = tk.Label(right_debug, text=\"Inference: --\",\n",
    "                              font=(\"Segoe UI\", 9), fg=\"#ffffff\", bg=\"#2a2a2a\")\n",
    "    inference_label.pack(anchor=\"w\")\n",
    "    \n",
    "    avg_inference_label = tk.Label(right_debug, text=\"Average: --\",\n",
    "                                  font=(\"Segoe UI\", 9), fg=\"#ffffff\", bg=\"#2a2a2a\")\n",
    "    avg_inference_label.pack(anchor=\"w\")\n",
    "    \n",
    "    percentage_label = tk.Label(right_debug, text=\"Distance: --\",\n",
    "                               font=(\"Segoe UI\", 9), fg=\"#ffffff\", bg=\"#2a2a2a\")\n",
    "    percentage_label.pack(anchor=\"w\")\n",
    "    \n",
    "    # Status and progress in debug panel\n",
    "    status_frame = tk.Frame(debug_frame, bg=\"#2a2a2a\")\n",
    "    status_frame.pack(fill=\"x\", padx=10, pady=5)\n",
    "    \n",
    "    status_label = tk.Label(status_frame, text=\"Race Not Started\",\n",
    "                           font=(\"Segoe UI\", 9, \"bold\"), fg=\"#95a5a6\", bg=\"#2a2a2a\")\n",
    "    status_label.pack()\n",
    "    \n",
    "    # Progress bar\n",
    "    try:\n",
    "        from tkinter import ttk\n",
    "        style = ttk.Style()\n",
    "        style.theme_use('clam')\n",
    "        style.configure(\"Custom.Horizontal.TProgressbar\",\n",
    "                       background='#2ecc71',\n",
    "                       troughcolor='#34495e',\n",
    "                       borderwidth=0,\n",
    "                       lightcolor='#2ecc71',\n",
    "                       darkcolor='#2ecc71')\n",
    "        \n",
    "        progress_frame = tk.Frame(debug_frame, bg=\"#2a2a2a\")\n",
    "        progress_frame.pack(fill=\"x\", padx=10, pady=5)\n",
    "        \n",
    "        progress_bar = ttk.Progressbar(progress_frame,\n",
    "                                      style=\"Custom.Horizontal.TProgressbar\",\n",
    "                                      length=200, mode='determinate',\n",
    "                                      maximum=99)\n",
    "        progress_bar.pack()\n",
    "        \n",
    "        progress_label = tk.Label(progress_frame, text=\"--\",\n",
    "                                 font=(\"Segoe UI\", 8), fg=\"#ffffff\", bg=\"#2a2a2a\")\n",
    "        progress_label.pack()\n",
    "        \n",
    "    except ImportError:\n",
    "        progress_bar = tk.Frame(debug_frame, bg=\"#34495e\", height=10)\n",
    "        progress_bar.pack(fill=\"x\", padx=10, pady=5)\n",
    "        progress_label = tk.Label(debug_frame, text=\"--\", bg=\"#2a2a2a\")\n",
    "    \n",
    "    # Control buttons in debug panel\n",
    "    control_debug_frame = tk.Frame(debug_frame, bg=\"#2a2a2a\")\n",
    "    control_debug_frame.pack(fill=\"x\", padx=10, pady=5)\n",
    "    \n",
    "    start_button = tk.Button(control_debug_frame, text=\"â–¶ï¸ Start\", command=start_capturing,\n",
    "                            bg=\"#27ae60\", fg=\"white\", font=(\"Segoe UI\", 9),\n",
    "                            relief=\"flat\", padx=8, pady=4)\n",
    "    start_button.pack(side=\"left\", padx=(0, 5))\n",
    "\n",
    "    stop_button = tk.Button(control_debug_frame, text=\"â¹ï¸ Stop\", command=stop_capturing,\n",
    "                           bg=\"#e74c3c\", fg=\"white\", font=(\"Segoe UI\", 9),\n",
    "                           relief=\"flat\", padx=8, pady=4)\n",
    "    stop_button.pack(side=\"left\")\n",
    "    \n",
    "    # Start the UI update loop\n",
    "    update_ui()\n",
    "    \n",
    "    # Make the window appear on top initially\n",
    "    root.lift()\n",
    "    root.focus_force()\n",
    "    \n",
    "    root.mainloop()\n",
    "\n",
    "# Initialize UI state variables\n",
    "is_pinned = False\n",
    "debug_expanded = False\n",
    "root = None\n",
    "progress_bar = None\n",
    "progress_label = None\n",
    "delta_display = \"+99.999\"\n",
    "\n",
    "ui_thread = threading.Thread(target=create_ui, daemon=True)\n",
    "ui_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the Trained Model using centralized system ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Try to load the configured model first, with fallback options\n",
    "model = None\n",
    "model_name = \"unknown\"\n",
    "\n",
    "try:\n",
    "    # First try the centralized model system\n",
    "    model = get_model()\n",
    "    model_name = get_default_model_type()\n",
    "    \n",
    "    # Try to load the optimized model weights\n",
    "    try:\n",
    "        model.load_state_dict(torch.load('percentage_cnn_optimized.pth', map_location=device))\n",
    "    except FileNotFoundError:\n",
    "        model.load_state_dict(torch.load('percentage_cnn.pth', map_location=device))\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "except Exception as e:\n",
    "    # Fallback to hardcoded model loading\n",
    "    try:\n",
    "        # Legacy SimpleCNN fallback\n",
    "        class SimpleCNN(nn.Module):\n",
    "            def __init__(self, num_classes=100):\n",
    "                super(SimpleCNN, self).__init__()\n",
    "                self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "                self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "                self.fc1 = nn.Linear(32 * 16 * 16, 512)\n",
    "                self.fc2 = nn.Linear(512, num_classes)\n",
    "                self.relu = nn.ReLU()\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.pool(self.relu(self.conv1(x)))\n",
    "                x = self.pool(self.relu(self.conv2(x)))\n",
    "                x = x.view(-1, 32 * 16 * 16)\n",
    "                x = self.relu(self.fc1(x))\n",
    "                x = self.fc2(x)\n",
    "                return x\n",
    "        \n",
    "        model = SimpleCNN(num_classes=100).to(device)\n",
    "        try:\n",
    "            model.load_state_dict(torch.load('percentage_cnn_optimized.pth', map_location=device))\n",
    "            model_name = \"SimpleCNN (fallback with optimized weights)\"\n",
    "        except FileNotFoundError:\n",
    "            model.load_state_dict(torch.load('percentage_cnn.pth', map_location=device))\n",
    "            model_name = \"SimpleCNN (fallback with legacy weights)\"\n",
    "    except Exception as fallback_error:\n",
    "        model = None\n",
    "\n",
    "if model is not None:\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # ðŸš€ PERFORMANCE OPTIMIZATIONS ðŸš€\n",
    "    \n",
    "    # 1. Enable cudnn benchmarking for consistent convolution algorithms\n",
    "    if device.type == 'cuda':\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "    \n",
    "    # 2. Disable gradient computation globally (already in eval mode, but this is extra)\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    # 3. Try to compile the model with torch.jit for optimization\n",
    "    try:\n",
    "        # Create a dummy input for scripting\n",
    "        dummy_input = torch.randn(1, 1, 64, 64).to(device)\n",
    "        model = torch.jit.script(model)\n",
    "        \n",
    "        # Warm up the compiled model\n",
    "        for _ in range(5):\n",
    "            with torch.no_grad():\n",
    "                _ = model(dummy_input)\n",
    "        \n",
    "    except Exception as jit_error:\n",
    "        pass  # Continue with eager mode\n",
    "    \n",
    "    # 4. Set memory allocation strategy\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# --- Define Image Transforms ---\n",
    "# These must be the same as the transforms used during training\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "textarray = []\n",
    "dist_box = None\n",
    "\n",
    "# CNN confidence threshold - adjust this value based on your model's performance\n",
    "CONFIDENCE_THRESHOLD = 0.65  # Reset bounding box if confidence is below this\n",
    "\n",
    "# Pre-allocate tensor for reuse (optimization)\n",
    "_tensor_cache = None\n",
    "\n",
    "# Timer extraction variables - simplified\n",
    "timer_box = None\n",
    "timer_roi_coords = None  # Cache for timer ROI coordinates\n",
    "last_percentage = None\n",
    "current_timer = None\n",
    "\n",
    "# Performance profiling variables\n",
    "profile_times = {\n",
    "    'frame_capture': [],\n",
    "    'timer_extraction': [],\n",
    "    'timer_skipped': [],  # Track when timer extraction is skipped\n",
    "    'ocr_search': [],\n",
    "    'cnn_prediction': [],\n",
    "    'ui_updates': [],\n",
    "    'other_processing': []\n",
    "}\n",
    "\n",
    "def find_timer_roi_coords(frame):\n",
    "    \"\"\"\n",
    "    Find timer ROI coordinates using the blue mask (BGR 228,0,0).\n",
    "    Returns the coordinates for extracting the timer ROI.\n",
    "    \"\"\"\n",
    "    # Crop to right half of the original frame\n",
    "    height, width = frame.shape[:2]\n",
    "    right_half = frame[:, int(width * 0.5):]\n",
    "    right_half_offset = int(width * 0.5)\n",
    "    \n",
    "    # Create blue mask (BGR 228,0,0) with tolerance\n",
    "    tolerance = 30\n",
    "    target_bgr = np.array([228, 0, 0])\n",
    "    lower_bgr = np.maximum(target_bgr - tolerance, 0)\n",
    "    upper_bgr = np.minimum(target_bgr + tolerance, 255)\n",
    "    blue_mask = cv2.inRange(right_half, lower_bgr, upper_bgr)\n",
    "    \n",
    "    # Find contours in the blue mask\n",
    "    contours, _ = cv2.findContours(blue_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        # Find the largest contour (should be the timer box)\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        area = cv2.contourArea(largest_contour)\n",
    "        \n",
    "        # Validate size (timer should be reasonably sized)\n",
    "        if w > 50 and h > 20 and area > 1000:\n",
    "            # No padding\n",
    "            x = max(0, x)\n",
    "            y = max(0, y)\n",
    "            w = min(right_half.shape[1] - x, w)\n",
    "            h = min(right_half.shape[0] - y, h)\n",
    "            \n",
    "            # Crop 10% from the left side\n",
    "            crop_left = int(w * 0.2)\n",
    "            \n",
    "            # Return coordinates relative to the full frame\n",
    "            return {\n",
    "                'x': x + right_half_offset + crop_left,\n",
    "                'y': y,\n",
    "                'w': w - crop_left,\n",
    "                'h': h\n",
    "            }\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_timer_roi_from_coords(frame, coords):\n",
    "    \"\"\"\n",
    "    Extract timer ROI using cached coordinates.\n",
    "    Returns a grayscale image with white background and black text.\n",
    "    \"\"\"\n",
    "    if coords is None:\n",
    "        return None\n",
    "    \n",
    "    # Crop to right half of the original frame\n",
    "    height, width = frame.shape[:2]\n",
    "    right_half = frame[:, int(width * 0.5):]\n",
    "    \n",
    "    # Create blue mask (BGR 228,0,0) with tolerance\n",
    "    tolerance = 30\n",
    "    target_bgr = np.array([228, 0, 0])\n",
    "    lower_bgr = np.maximum(target_bgr - tolerance, 0)\n",
    "    upper_bgr = np.minimum(target_bgr + tolerance, 255)\n",
    "    blue_mask = cv2.inRange(right_half, lower_bgr, upper_bgr)\n",
    "    \n",
    "    # Calculate coordinates relative to right_half\n",
    "    right_half_offset = int(width * 0.5)\n",
    "    rel_x = coords['x'] - right_half_offset\n",
    "    rel_y = coords['y']\n",
    "    rel_w = coords['w']\n",
    "    rel_h = coords['h']\n",
    "    \n",
    "    # Ensure coordinates are within bounds\n",
    "    rel_x = max(0, min(rel_x, right_half.shape[1] - 1))\n",
    "    rel_y = max(0, min(rel_y, right_half.shape[0] - 1))\n",
    "    rel_w = min(rel_w, right_half.shape[1] - rel_x)\n",
    "    rel_h = min(rel_h, right_half.shape[0] - rel_y)\n",
    "    \n",
    "    if rel_w > 0 and rel_h > 0:\n",
    "        # Extract the timer ROI from the blue mask\n",
    "        timer_roi_mask = blue_mask[rel_y:rel_y+rel_h, rel_x:rel_x+rel_w]\n",
    "        \n",
    "        # The blue mask has white pixels where blue background is detected\n",
    "        # We want white background with black text, so we use the mask directly\n",
    "        # Blue background areas become white (255), text areas become black (0)\n",
    "        timer_roi_corrected = timer_roi_mask.copy()\n",
    "        \n",
    "        return timer_roi_corrected\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_timer_roi(timer_roi):\n",
    "    \"\"\"\n",
    "    Process the timer ROI using template matching and convert to milliseconds.\n",
    "    Optimized for performance - no image saving, visualization, or OCR fallback.\n",
    "    \"\"\"\n",
    "    if timer_roi is None or timer_roi.size == 0:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Use template matching for digit recognition only\n",
    "        if digit_templates:\n",
    "            digits_string, digit_details, processed_img = extract_digits_from_timer(timer_roi, digit_templates, debug=False)\n",
    "            \n",
    "            # Convert to total milliseconds if we have enough digits\n",
    "            total_ms = convert_timer_to_milliseconds(digits_string)\n",
    "            \n",
    "            if digits_string:\n",
    "                # Print timer information each iteration\n",
    "                if total_ms is not None:\n",
    "                    minutes = total_ms // 60000\n",
    "                    seconds = (total_ms % 60000) // 1000\n",
    "                    milliseconds = total_ms % 1000\n",
    "                    print(f\"Timer at {last_percentage}: {digits_string} -> {minutes:02d}:{seconds:02d}.{milliseconds:03d} ({total_ms}ms)\")\n",
    "                else:\n",
    "                    print(f\"Timer at {last_percentage}: {digits_string} (conversion failed)\")\n",
    "                    \n",
    "                return digits_string\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def predict_with_cnn(image_array):\n",
    "    \"\"\"\n",
    "    Use the trained CNN to predict the percentage from an image array.\n",
    "    \n",
    "    Args:\n",
    "        image_array: numpy array of the preprocessed image\n",
    "        \n",
    "    Returns:\n",
    "        predicted_percentage: integer from 0-99, or None if prediction fails\n",
    "    \"\"\"\n",
    "    global inference_times, avg_inference_time, _tensor_cache\n",
    "    \n",
    "    if model is None:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Start timing - more precise timing\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()  # Ensure all previous operations are complete\n",
    "        inference_start = systime.perf_counter()\n",
    "        \n",
    "        # Convert numpy array to PIL Image\n",
    "        pil_image = Image.fromarray(image_array)\n",
    "        \n",
    "        # Apply transforms\n",
    "        tensor_image = data_transforms(pil_image)\n",
    "        \n",
    "        # Reuse tensor cache if possible (optimization)\n",
    "        if _tensor_cache is None or _tensor_cache.shape[0] != 1:\n",
    "            _tensor_cache = tensor_image.unsqueeze(0).to(device, non_blocking=True)\n",
    "        else:\n",
    "            _tensor_cache.copy_(tensor_image.unsqueeze(0), non_blocking=True)\n",
    "        \n",
    "        # Make prediction with minimal overhead\n",
    "        outputs = model(_tensor_cache)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        confidence = torch.softmax(outputs, 1)[0][predicted].item()\n",
    "        \n",
    "        # End timing with synchronization\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()  # Wait for GPU operations to complete\n",
    "        inference_end = systime.perf_counter()\n",
    "        \n",
    "        inference_time = (inference_end - inference_start) * 1000  # Convert to ms\n",
    "        \n",
    "        # Update inference time tracking\n",
    "        inference_times.append(inference_time)\n",
    "        if len(inference_times) > 100:  # Keep only last 100 measurements\n",
    "            inference_times.pop(0)\n",
    "        \n",
    "        # Calculate new average\n",
    "        new_avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "        avg_inference_time = new_avg_inference_time\n",
    "            \n",
    "        return predicted.item(), confidence\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def print_performance_summary():\n",
    "    \"\"\"Print average times for each profiled stage\"\"\"\n",
    "    print(\"\\n=== PERFORMANCE PROFILING SUMMARY ===\")\n",
    "    for stage, times in profile_times.items():\n",
    "        if times:\n",
    "            avg_time = sum(times) / len(times)\n",
    "            print(f\"{stage.replace('_', ' ').title()}: {avg_time:.2f}ms avg ({len(times)} samples)\")\n",
    "        else:\n",
    "            print(f\"{stage.replace('_', ' ').title()}: No data\")\n",
    "    print(\"=====================================\\n\")\n",
    "\n",
    "def the_loop():\n",
    "    global dist_box, capturing, textarray, camera, percentage, elapsed_ms, total_loops, race_in_progress\n",
    "    global timer_box, timer_roi_coords, last_percentage, current_timer\n",
    "    global current_timer_ms, current_timer_display, loop_times, avg_loop_time, profile_times\n",
    "\n",
    "    # Start the loop\n",
    "    while capturing:\n",
    "        if capturing:\n",
    "            # Start timing the entire loop\n",
    "            loop_start_time = systime.perf_counter()\n",
    "            total_loops += 1\n",
    "            \n",
    "            # === STAGE 1: FRAME CAPTURE ===\n",
    "            stage_start = systime.perf_counter()\n",
    "            window = camera.get_latest_frame()\n",
    "            height, width, _ = window.shape\n",
    "            top_right_region = window[50:height, 0:int(width * 0.35)]\n",
    "            stage_end = systime.perf_counter()\n",
    "            profile_times['frame_capture'].append((stage_end - stage_start) * 1000)\n",
    "\n",
    "            # === STAGE 2: TIMER ROI COORDINATE TRACKING (ALWAYS RUN) ===\n",
    "            # Always update timer ROI coordinates to keep track of timer location\n",
    "            if timer_roi_coords is None:\n",
    "                timer_roi_coords = find_timer_roi_coords(window)\n",
    "\n",
    "            # === STAGE 3: OCR SEARCH (when needed) ===\n",
    "            stage_start = systime.perf_counter()\n",
    "            current_percentage_value = None\n",
    "            if dist_box is None:\n",
    "                # Update race state - searching for race\n",
    "                race_in_progress = False\n",
    "                \n",
    "                # Recalculate timer ROI coordinates when dist_box is None (re-searching for race)\n",
    "                timer_roi_coords = find_timer_roi_coords(window)\n",
    "                \n",
    "                preprocessed_region = pre_process(top_right_region)\n",
    "                results = reader.readtext(preprocessed_region)\n",
    "                \n",
    "                dist_found = False\n",
    "                dist_bbox = None\n",
    "                dist_index = -1\n",
    "                \n",
    "                # Find DIST\n",
    "                for i, (bbox, text, confidence) in enumerate(results):\n",
    "                    if \"dist\" in text.lower() and not dist_found:\n",
    "                        dist_bbox = np.array(bbox)\n",
    "                        dist_index = i\n",
    "                        dist_found = True\n",
    "                \n",
    "                # If we found DIST, look for percentage\n",
    "                if dist_found:\n",
    "                    dist_x0, dist_y0 = np.min(dist_bbox[:, 0]), np.min(dist_bbox[:, 1])\n",
    "                    dist_x1, dist_y1 = np.max(dist_bbox[:, 0]), np.max(dist_bbox[:, 1])\n",
    "                    dist_center_y = (dist_y0 + dist_y1) / 2\n",
    "                    \n",
    "                    best_percentage_match = None\n",
    "                    best_score = 0\n",
    "                    \n",
    "                    # Look for percentage indicators with more flexible criteria\n",
    "                    for j, (bbox, text, confidence) in enumerate(results):\n",
    "                        if j == dist_index:  # Skip the DIST box itself\n",
    "                            continue\n",
    "                            \n",
    "                        bbox_array = np.array(bbox)\n",
    "                        nx0, ny0 = np.min(bbox_array[:, 0]), np.min(bbox_array[:, 1])\n",
    "                        nx1, ny1 = np.max(bbox_array[:, 0]), np.max(bbox_array[:, 1])\n",
    "                        bbox_center_y = (ny0 + ny1) / 2\n",
    "                        \n",
    "                        # More flexible matching criteria\n",
    "                        text_clean = text.strip().replace(' ', '').replace(',', '').replace('.', '')\n",
    "                        \n",
    "                        # Check if it looks like a percentage\n",
    "                        has_percent = '%' in text_clean\n",
    "                        has_numbers = any(char.isdigit() for char in text_clean)\n",
    "                        ends_with_7 = text_clean.endswith('7')  # Sometimes % is read as 7\n",
    "                        \n",
    "                        # Position criteria (more flexible)\n",
    "                        reasonable_y_distance = abs(bbox_center_y - dist_center_y) < 50\n",
    "                        to_the_right = nx0 > dist_x0\n",
    "                        reasonable_x_distance = (nx0 - dist_x1) < 200\n",
    "                        \n",
    "                        # Calculate a score for this match\n",
    "                        score = 0\n",
    "                        if has_percent:\n",
    "                            score += 50\n",
    "                        if has_numbers:\n",
    "                            score += 20\n",
    "                        if ends_with_7:\n",
    "                            score += 10\n",
    "                        if reasonable_y_distance:\n",
    "                            score += 30\n",
    "                        if to_the_right:\n",
    "                            score += 20\n",
    "                        if reasonable_x_distance:\n",
    "                            score += 10\n",
    "                        \n",
    "                        # Add confidence boost\n",
    "                        score += confidence * 10\n",
    "                        \n",
    "                        if score > best_score and score > 40:\n",
    "                            best_score = score\n",
    "                            best_percentage_match = (j, bbox, text, confidence)\n",
    "                    \n",
    "                    # If we found a good percentage match\n",
    "                    if best_percentage_match is not None:\n",
    "                        j, next_bbox, next_text, next_confidence = best_percentage_match\n",
    "                        \n",
    "                        # Calculate combined bounding box\n",
    "                        next_box = np.array(next_bbox)\n",
    "                        nx0, ny0 = np.min(next_box[:, 0]), np.min(next_box[:, 1])\n",
    "                        nx1, ny1 = np.max(next_box[:, 0]), np.max(next_box[:, 1])\n",
    "                        \n",
    "                        # Extend bounding box to include both with some padding\n",
    "                        x0 = int(min(dist_x0, nx0)) - 5\n",
    "                        y0 = int(min(dist_y0, ny0)) - 5\n",
    "                        x1 = int(max(dist_x1, nx1)) + 5\n",
    "                        y1 = int(max(dist_y1, ny1)) + 5\n",
    "                        \n",
    "                        # Ensure bounds are within image\n",
    "                        x0 = max(0, x0)\n",
    "                        y0 = max(0, y0)\n",
    "                        x1 = min(top_right_region.shape[1], x1)\n",
    "                        y1 = min(top_right_region.shape[0], y1)\n",
    "                    else:\n",
    "                        # Fallback: just use DIST box with some expansion\n",
    "                        x0 = int(dist_x0) - 10\n",
    "                        y0 = int(dist_y0) - 10\n",
    "                        x1 = int(dist_x1) + 100\n",
    "                        y1 = int(dist_y1) + 30\n",
    "                        \n",
    "                        # Ensure bounds are within image\n",
    "                        x0 = max(0, x0)\n",
    "                        y0 = max(0, y0)\n",
    "                        x1 = min(top_right_region.shape[1], x1)\n",
    "                        y1 = min(top_right_region.shape[0], y1)\n",
    "                    \n",
    "                    # Create the final bounding box\n",
    "                    dist_box = np.array([[x0, y0], [x1, y0], [x1, y1], [x0, y1]])\n",
    "            stage_end = systime.perf_counter()\n",
    "            if dist_box is None:  # Only record OCR time when it actually runs\n",
    "                profile_times['ocr_search'].append((stage_end - stage_start) * 1000)\n",
    "            \n",
    "            # === STAGE 4: OTHER PROCESSING ===\n",
    "            stage_start = systime.perf_counter()\n",
    "            clear_output(wait=True)\n",
    "            stage_end = systime.perf_counter()\n",
    "            profile_times['other_processing'].append((stage_end - stage_start) * 1000)\n",
    "            \n",
    "            # === STAGE 5: CNN PREDICTION ===\n",
    "            stage_start = systime.perf_counter()\n",
    "            cnn_result = None\n",
    "            # If we have the bounding box, crop the image\n",
    "            if dist_box is not None:\n",
    "                # Update race state - race is in progress\n",
    "                race_in_progress = True\n",
    "                \n",
    "                roi = top_right_region[int(dist_box[0][1]):int(dist_box[2][1]), int(dist_box[0][0]):int(dist_box[1][0])]\n",
    "                roi = roi[:, int(roi.shape[1] * 23 / 40):]\n",
    "\n",
    "                # Preprocess the cropped image for CNN\n",
    "                preprocessed_region = pre_process_distbox(roi, for_cnn=True)\n",
    "\n",
    "                # Use CNN for recognition\n",
    "                cnn_result = predict_with_cnn(preprocessed_region)\n",
    "            stage_end = systime.perf_counter()\n",
    "            if dist_box is not None:  # Only record CNN time when it actually runs\n",
    "                profile_times['cnn_prediction'].append((stage_end - stage_start) * 1000)\n",
    "\n",
    "            # Process CNN prediction and determine if we need to extract timer\n",
    "            percentage_changed = False\n",
    "            try:\n",
    "                if cnn_result is not None:\n",
    "                    predicted_percentage, confidence = cnn_result\n",
    "                    text2 = f\"{predicted_percentage}%\"\n",
    "                    current_percentage_value = predicted_percentage\n",
    "                    \n",
    "                    # Check if percentage changed\n",
    "                    if last_percentage != predicted_percentage:\n",
    "                        percentage_changed = True\n",
    "                        last_percentage = predicted_percentage\n",
    "                        print(f\"Percentage changed to: {predicted_percentage}%\")\n",
    "                    \n",
    "                    percentage = text2\n",
    "\n",
    "                    # Reset bounding box if confidence is too low\n",
    "                    if confidence < CONFIDENCE_THRESHOLD:\n",
    "                        dist_box = None\n",
    "                else:\n",
    "                    dist_box = None\n",
    "            except Exception as e:\n",
    "                dist_box = None\n",
    "\n",
    "            # === STAGE 6: TIMER EXTRACTION (ONLY WHEN PERCENTAGE CHANGES) ===\n",
    "            stage_start = systime.perf_counter()\n",
    "            if percentage_changed and timer_roi_coords is not None:\n",
    "                # Percentage increased - extract timer at this milestone\n",
    "                timer_roi = extract_timer_roi_from_coords(window, timer_roi_coords)\n",
    "                if timer_roi is not None:\n",
    "                    extracted_timer = process_timer_roi(timer_roi)\n",
    "                    if extracted_timer:\n",
    "                        current_timer = extracted_timer\n",
    "                        \n",
    "                        # Convert to milliseconds and update display\n",
    "                        timer_ms = convert_timer_to_milliseconds(extracted_timer)\n",
    "                        if timer_ms is not None:\n",
    "                            current_timer_ms = timer_ms\n",
    "                            # Format for display: MM:SS.mmm\n",
    "                            minutes = timer_ms // 60000\n",
    "                            seconds = (timer_ms % 60000) // 1000\n",
    "                            milliseconds = timer_ms % 1000\n",
    "                            current_timer_display = f\"{minutes:02d}:{seconds:02d}.{milliseconds:03d}\"\n",
    "                \n",
    "                stage_end = systime.perf_counter()\n",
    "                profile_times['timer_extraction'].append((stage_end - stage_start) * 1000)\n",
    "            else:\n",
    "                # Timer extraction skipped - track this for performance analysis\n",
    "                stage_end = systime.perf_counter()\n",
    "                profile_times['timer_skipped'].append((stage_end - stage_start) * 1000)\n",
    "            \n",
    "            # End timing the entire loop\n",
    "            loop_end_time = systime.perf_counter()\n",
    "            elapsed_ms = (loop_end_time - loop_start_time) * 1000\n",
    "            \n",
    "            # Update loop time tracking with running average (30 samples)\n",
    "            loop_times.append(elapsed_ms)\n",
    "            if len(loop_times) > 30:  # Keep last 30 measurements for running average\n",
    "                loop_times.pop(0)\n",
    "            \n",
    "            # Calculate new average loop time\n",
    "            avg_loop_time = sum(loop_times) / len(loop_times)\n",
    "\n",
    "            systime.sleep(0.001)\n",
    "    \n",
    "    # Print performance summary when loop ends\n",
    "    print_performance_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main loop\n",
    "the_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
