{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb913cb",
   "metadata": {},
   "source": [
    "# Dataset Classification with Tesseract\n",
    "\n",
    "This notebook processes the images in the `dataset` folder, classifies them using Tesseract OCR, renames the files based on their classification, and provides summary metrics.\n",
    "\n",
    "**IMPORTANT:** Before running, you must have Google's Tesseract OCR engine installed on your system and accessible in your PATH. You can find installation instructions here: [https://github.com/tesseract-ocr/tesseract](https://github.com/tesseract-ocr/tesseract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pytesseract\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# If tesseract is not in your PATH, you can uncomment the following line\n",
    "# and provide the path to your tesseract.exe\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset\"\n",
    "classification_metrics = Counter()\n",
    "file_rename_counter = Counter()\n",
    "unclassified_count = 0\n",
    "\n",
    "# List all files in the dataset directory\n",
    "try:\n",
    "    image_files = [f for f in os.listdir(DATASET_DIR) if f.endswith('.png')]\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The directory '{DATASET_DIR}' was not found.\")\n",
    "    image_files = []\n",
    "\n",
    "print(f\"Found {len(image_files)} images to process.\")\n",
    "\n",
    "for filename in image_files:\n",
    "    old_filepath = os.path.join(DATASET_DIR, filename)\n",
    "    \n",
    "    # Load the image in grayscale\n",
    "    image = cv2.imread(old_filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Configure Tesseract\n",
    "    # --psm 7: Treat the image as a single text line.\n",
    "    # -c tessedit_char_whitelist: Restrict output to these characters.\n",
    "    config = \"--psm 7 -c tessedit_char_whitelist=0123456789%\"\n",
    "    \n",
    "    # Perform OCR\n",
    "    text = pytesseract.image_to_string(image, config=config).strip()\n",
    "    \n",
    "    # --- Text Cleanup and Validation ---\n",
    "    # Find the most likely percentage value\n",
    "    match = re.search(r'(\\d+)', text)\n",
    "    if match:\n",
    "        clean_text = match.group(1) + \"%\"\n",
    "    else:\n",
    "        clean_text = \"\"\n",
    "\n",
    "    # --- File Renaming ---\n",
    "    if clean_text:\n",
    "        # Update the classification metrics\n",
    "        classification_metrics[clean_text] += 1\n",
    "        \n",
    "        # Get the instance count for the new filename\n",
    "        file_rename_counter[clean_text] += 1\n",
    "        instance_count = file_rename_counter[clean_text]\n",
    "        \n",
    "        # Sanitize '%' for the filename, as it can cause issues on some systems\n",
    "        safe_label = clean_text.replace('%', 'pct')\n",
    "        new_filename = f\"{safe_label}_{instance_count}.png\"\n",
    "        new_filepath = os.path.join(DATASET_DIR, new_filename)\n",
    "        \n",
    "        try:\n",
    "            # Rename the file\n",
    "            os.rename(old_filepath, new_filepath)\n",
    "            print(f\"Renamed '{filename}' to '{new_filename}'\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error renaming file {filename}: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Could not classify '{filename}', skipping rename.\")\n",
    "        unclassified_count += 1\n",
    "\n",
    "print(\"\\n--- Processing Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907eaacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display Classification Metrics ---\n",
    "\n",
    "print(\"Tesseract Classification Metrics:\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "if classification_metrics:\n",
    "    total_classified = sum(classification_metrics.values())\n",
    "    print(f\"Total Images Classified: {total_classified}\")\n",
    "    print(f\"Total Images Unclassified: {unclassified_count}\")\n",
    "    print(f\"Number of Unique Classes: {len(classification_metrics)}\")\n",
    "    print(\"\\n--- Classification Counts ---\")\n",
    "    \n",
    "    # Sort by most common\n",
    "    for label, count in classification_metrics.most_common():\n",
    "        print(f\"- {label}: {count} times\")\n",
    "else:\n",
    "    print(\"No images were successfully classified.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0299a9dc",
   "metadata": {},
   "source": [
    "# Standardize Dataset Naming\n",
    "\n",
    "The following cell processes the images in the `dataset` folder, standardizes their filenames based on the numeric prefix, and copies them to a new `dataset_cleaned` directory. This is useful for creating a clean, consistently named dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a97d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: dataset_cleaned\n",
      "Found 419 images to standardize.\n",
      "\n",
      "--- Standardization Complete ---\n",
      "Successfully processed and copied 419 files.\n",
      "Skipped 0 files.\n",
      "\n",
      "--- Standardization Complete ---\n",
      "Successfully processed and copied 419 files.\n",
      "Skipped 0 files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "# Define directories\n",
    "SOURCE_DIR = \"dataset\"\n",
    "DEST_DIR = \"dataset_cleaned\"\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "print(f\"Created directory: {DEST_DIR}\")\n",
    "\n",
    "# Counter for new filenames\n",
    "rename_counter = Counter()\n",
    "files_processed = 0\n",
    "files_skipped = 0\n",
    "\n",
    "# List all files in the source directory\n",
    "try:\n",
    "    image_files = [f for f in os.listdir(SOURCE_DIR) if f.endswith('.png')]\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The source directory '{SOURCE_DIR}' was not found.\")\n",
    "    image_files = []\n",
    "\n",
    "print(f\"Found {len(image_files)} images to standardize.\")\n",
    "\n",
    "# Process each file\n",
    "for filename in image_files:\n",
    "    # Extract the numeric label from the start of the filename\n",
    "    match = re.match(r'^(\\d+)', filename)\n",
    "    \n",
    "    if match:\n",
    "        label = match.group(1)\n",
    "        \n",
    "        # Increment the counter for this label\n",
    "        rename_counter[label] += 1\n",
    "        instance_count = rename_counter[label]\n",
    "        \n",
    "        # Create the new standardized filename\n",
    "        new_filename = f\"{label}pct_{instance_count}.png\"\n",
    "        \n",
    "        # Define full paths\n",
    "        old_filepath = os.path.join(SOURCE_DIR, filename)\n",
    "        new_filepath = os.path.join(DEST_DIR, new_filename)\n",
    "        \n",
    "        # Copy the file to the new directory with the new name\n",
    "        shutil.copy(old_filepath, new_filepath)\n",
    "        files_processed += 1\n",
    "    else:\n",
    "        print(f\"Could not extract label from '{filename}', skipping.\")\n",
    "        files_skipped += 1\n",
    "\n",
    "print(\"\\n--- Standardization Complete ---\")\n",
    "print(f\"Successfully processed and copied {files_processed} files.\")\n",
    "print(f\"Skipped {files_skipped} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1021d7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directory: dataset_merged\n",
      "==================================================\n",
      "MERGING DATASET FOLDERS\n",
      "==================================================\n",
      "\n",
      "Processing 419 files from Folder 1 (dataset_cleaned)...\n",
      "Merged 100 files...\n",
      "Merged 200 files...\n",
      "Merged 300 files...\n",
      "Merged 200 files...\n",
      "Merged 300 files...\n",
      "Merged 400 files...\n",
      "\n",
      "Processing 1497 files from Folder 2...\n",
      "Merged 500 files...\n",
      "Merged 400 files...\n",
      "\n",
      "Processing 1497 files from Folder 2...\n",
      "Merged 500 files...\n",
      "Merged 600 files...\n",
      "Merged 600 files...\n",
      "Merged 700 files...\n",
      "Merged 700 files...\n",
      "Merged 800 files...\n",
      "Merged 900 files...\n",
      "Merged 800 files...\n",
      "Merged 900 files...\n",
      "Merged 1000 files...\n",
      "Merged 1000 files...\n",
      "Merged 1100 files...\n",
      "Merged 1100 files...\n",
      "Merged 1200 files...\n",
      "Merged 1200 files...\n",
      "Merged 1300 files...\n",
      "Merged 1300 files...\n",
      "Merged 1400 files...\n",
      "Merged 1400 files...\n",
      "Merged 1500 files...\n",
      "Merged 1500 files...\n",
      "Merged 1600 files...\n",
      "Merged 1600 files...\n",
      "Merged 1700 files...\n",
      "Merged 1700 files...\n",
      "Merged 1800 files...\n",
      "Merged 1800 files...\n",
      "Merged 1900 files...\n",
      "\n",
      "--- Merging Complete ---\n",
      "Total files merged: 1916\n",
      "Output directory: dataset_merged\n",
      "Number of unique labels: 100\n",
      "\n",
      "--- Label Distribution ---\n",
      "Label 0: 60 images\n",
      "Label 1: 30 images\n",
      "Label 2: 26 images\n",
      "Label 3: 22 images\n",
      "Label 4: 22 images\n",
      "Label 5: 24 images\n",
      "Label 6: 29 images\n",
      "Label 7: 39 images\n",
      "Label 8: 28 images\n",
      "Label 9: 23 images\n",
      "Label 10: 25 images\n",
      "Label 11: 14 images\n",
      "Label 12: 9 images\n",
      "Label 13: 7 images\n",
      "Label 14: 10 images\n",
      "Label 15: 9 images\n",
      "Label 16: 13 images\n",
      "Label 17: 13 images\n",
      "Label 18: 10 images\n",
      "Label 19: 17 images\n",
      "Label 20: 13 images\n",
      "Label 21: 16 images\n",
      "Label 22: 14 images\n",
      "Label 23: 16 images\n",
      "Label 24: 17 images\n",
      "Label 25: 17 images\n",
      "Label 26: 15 images\n",
      "Label 27: 15 images\n",
      "Label 28: 15 images\n",
      "Label 29: 15 images\n",
      "Label 30: 18 images\n",
      "Label 31: 15 images\n",
      "Label 32: 14 images\n",
      "Label 33: 14 images\n",
      "Label 34: 13 images\n",
      "Label 35: 13 images\n",
      "Label 36: 14 images\n",
      "Label 37: 17 images\n",
      "Label 38: 22 images\n",
      "Label 39: 20 images\n",
      "Label 40: 20 images\n",
      "Label 41: 18 images\n",
      "Label 42: 18 images\n",
      "Label 43: 16 images\n",
      "Label 44: 16 images\n",
      "Label 45: 19 images\n",
      "Label 46: 19 images\n",
      "Label 47: 18 images\n",
      "Label 48: 18 images\n",
      "Label 49: 18 images\n",
      "Label 50: 22 images\n",
      "Label 51: 22 images\n",
      "Label 52: 18 images\n",
      "Label 53: 18 images\n",
      "Label 54: 29 images\n",
      "Label 55: 16 images\n",
      "Label 56: 22 images\n",
      "Label 57: 16 images\n",
      "Label 58: 22 images\n",
      "Label 59: 22 images\n",
      "Label 60: 19 images\n",
      "Label 61: 18 images\n",
      "Label 62: 21 images\n",
      "Label 63: 18 images\n",
      "Label 64: 22 images\n",
      "Label 65: 20 images\n",
      "Label 66: 20 images\n",
      "Label 67: 22 images\n",
      "Label 68: 32 images\n",
      "Label 69: 14 images\n",
      "Label 70: 19 images\n",
      "Label 71: 18 images\n",
      "Label 72: 18 images\n",
      "Label 73: 17 images\n",
      "Label 74: 23 images\n",
      "Label 75: 24 images\n",
      "Label 76: 21 images\n",
      "Label 77: 26 images\n",
      "Label 78: 19 images\n",
      "Label 79: 21 images\n",
      "Label 80: 20 images\n",
      "Label 81: 19 images\n",
      "Label 82: 17 images\n",
      "Label 83: 19 images\n",
      "Label 84: 16 images\n",
      "Label 85: 20 images\n",
      "Label 86: 18 images\n",
      "Label 87: 20 images\n",
      "Label 88: 19 images\n",
      "Label 89: 17 images\n",
      "Label 90: 18 images\n",
      "Label 91: 22 images\n",
      "Label 92: 19 images\n",
      "Label 93: 17 images\n",
      "Label 94: 19 images\n",
      "Label 95: 16 images\n",
      "Label 96: 19 images\n",
      "Label 97: 21 images\n",
      "Label 98: 17 images\n",
      "Label 99: 21 images\n",
      "Merged 1900 files...\n",
      "\n",
      "--- Merging Complete ---\n",
      "Total files merged: 1916\n",
      "Output directory: dataset_merged\n",
      "Number of unique labels: 100\n",
      "\n",
      "--- Label Distribution ---\n",
      "Label 0: 60 images\n",
      "Label 1: 30 images\n",
      "Label 2: 26 images\n",
      "Label 3: 22 images\n",
      "Label 4: 22 images\n",
      "Label 5: 24 images\n",
      "Label 6: 29 images\n",
      "Label 7: 39 images\n",
      "Label 8: 28 images\n",
      "Label 9: 23 images\n",
      "Label 10: 25 images\n",
      "Label 11: 14 images\n",
      "Label 12: 9 images\n",
      "Label 13: 7 images\n",
      "Label 14: 10 images\n",
      "Label 15: 9 images\n",
      "Label 16: 13 images\n",
      "Label 17: 13 images\n",
      "Label 18: 10 images\n",
      "Label 19: 17 images\n",
      "Label 20: 13 images\n",
      "Label 21: 16 images\n",
      "Label 22: 14 images\n",
      "Label 23: 16 images\n",
      "Label 24: 17 images\n",
      "Label 25: 17 images\n",
      "Label 26: 15 images\n",
      "Label 27: 15 images\n",
      "Label 28: 15 images\n",
      "Label 29: 15 images\n",
      "Label 30: 18 images\n",
      "Label 31: 15 images\n",
      "Label 32: 14 images\n",
      "Label 33: 14 images\n",
      "Label 34: 13 images\n",
      "Label 35: 13 images\n",
      "Label 36: 14 images\n",
      "Label 37: 17 images\n",
      "Label 38: 22 images\n",
      "Label 39: 20 images\n",
      "Label 40: 20 images\n",
      "Label 41: 18 images\n",
      "Label 42: 18 images\n",
      "Label 43: 16 images\n",
      "Label 44: 16 images\n",
      "Label 45: 19 images\n",
      "Label 46: 19 images\n",
      "Label 47: 18 images\n",
      "Label 48: 18 images\n",
      "Label 49: 18 images\n",
      "Label 50: 22 images\n",
      "Label 51: 22 images\n",
      "Label 52: 18 images\n",
      "Label 53: 18 images\n",
      "Label 54: 29 images\n",
      "Label 55: 16 images\n",
      "Label 56: 22 images\n",
      "Label 57: 16 images\n",
      "Label 58: 22 images\n",
      "Label 59: 22 images\n",
      "Label 60: 19 images\n",
      "Label 61: 18 images\n",
      "Label 62: 21 images\n",
      "Label 63: 18 images\n",
      "Label 64: 22 images\n",
      "Label 65: 20 images\n",
      "Label 66: 20 images\n",
      "Label 67: 22 images\n",
      "Label 68: 32 images\n",
      "Label 69: 14 images\n",
      "Label 70: 19 images\n",
      "Label 71: 18 images\n",
      "Label 72: 18 images\n",
      "Label 73: 17 images\n",
      "Label 74: 23 images\n",
      "Label 75: 24 images\n",
      "Label 76: 21 images\n",
      "Label 77: 26 images\n",
      "Label 78: 19 images\n",
      "Label 79: 21 images\n",
      "Label 80: 20 images\n",
      "Label 81: 19 images\n",
      "Label 82: 17 images\n",
      "Label 83: 19 images\n",
      "Label 84: 16 images\n",
      "Label 85: 20 images\n",
      "Label 86: 18 images\n",
      "Label 87: 20 images\n",
      "Label 88: 19 images\n",
      "Label 89: 17 images\n",
      "Label 90: 18 images\n",
      "Label 91: 22 images\n",
      "Label 92: 19 images\n",
      "Label 93: 17 images\n",
      "Label 94: 19 images\n",
      "Label 95: 16 images\n",
      "Label 96: 19 images\n",
      "Label 97: 21 images\n",
      "Label 98: 17 images\n",
      "Label 99: 21 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# --- Dataset Merging Configuration ---\n",
    "# Change these folder names as needed\n",
    "FOLDER1 = \"dataset_cleaned\"  # First folder to merge\n",
    "FOLDER2 = \"dataset_cleaned2\"  # Second folder to merge - change this to your second folder\n",
    "OUTPUT_FOLDER = \"dataset_merged\"\n",
    "\n",
    "# Create the output directory\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "print(f\"Created output directory: {OUTPUT_FOLDER}\")\n",
    "\n",
    "# Counter to track instances of each label across both folders\n",
    "global_counter = Counter()\n",
    "total_files_merged = 0\n",
    "\n",
    "def merge_folder(source_folder, folder_name):\n",
    "    \"\"\"\n",
    "    Merge files from a source folder into the output folder.\n",
    "    \n",
    "    Args:\n",
    "        source_folder: Path to the source folder\n",
    "        folder_name: Name of the folder (for logging purposes)\n",
    "    \"\"\"\n",
    "    global global_counter, total_files_merged\n",
    "    \n",
    "    if not os.path.exists(source_folder):\n",
    "        print(f\"Warning: {source_folder} does not exist, skipping.\")\n",
    "        return\n",
    "    \n",
    "    files_in_folder = [f for f in os.listdir(source_folder) if f.endswith('.png')]\n",
    "    print(f\"\\nProcessing {len(files_in_folder)} files from {folder_name}...\")\n",
    "    \n",
    "    for filename in files_in_folder:\n",
    "        # Extract the numeric label from the filename\n",
    "        match = re.match(r'^(\\d+)', filename)\n",
    "        \n",
    "        if match:\n",
    "            label = match.group(1)\n",
    "            \n",
    "            # Increment the global counter for this label\n",
    "            global_counter[label] += 1\n",
    "            instance_count = global_counter[label]\n",
    "            \n",
    "            # Create the new standardized filename\n",
    "            new_filename = f\"{label}pct_{instance_count}.png\"\n",
    "            \n",
    "            # Define full paths\n",
    "            old_filepath = os.path.join(source_folder, filename)\n",
    "            new_filepath = os.path.join(OUTPUT_FOLDER, new_filename)\n",
    "            \n",
    "            # Copy the file with the new name\n",
    "            shutil.copy(old_filepath, new_filepath)\n",
    "            total_files_merged += 1\n",
    "            \n",
    "            if total_files_merged % 100 == 0:  # Progress update every 100 files\n",
    "                print(f\"Merged {total_files_merged} files...\")\n",
    "        else:\n",
    "            print(f\"Could not extract label from '{filename}', skipping.\")\n",
    "\n",
    "# Merge both folders\n",
    "print(\"=\"*50)\n",
    "print(\"MERGING DATASET FOLDERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "merge_folder(FOLDER1, \"Folder 1 (dataset_cleaned)\")\n",
    "merge_folder(FOLDER2, \"Folder 2\")\n",
    "\n",
    "print(f\"\\n--- Merging Complete ---\")\n",
    "print(f\"Total files merged: {total_files_merged}\")\n",
    "print(f\"Output directory: {OUTPUT_FOLDER}\")\n",
    "print(f\"Number of unique labels: {len(global_counter)}\")\n",
    "\n",
    "# Display label distribution\n",
    "print(\"\\n--- Label Distribution ---\")\n",
    "for label, count in sorted(global_counter.items(), key=lambda x: int(x[0])):\n",
    "    print(f\"Label {label}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02a58c",
   "metadata": {},
   "source": [
    "# CNN Training with PyTorch\n",
    "\n",
    "This section defines and trains a Convolutional Neural Network (CNN) on the cleaned dataset.\n",
    "The process includes:\n",
    "1.  A custom `Dataset` class to load images and parse labels from filenames.\n",
    "2.  Splitting the data into training and validation sets.\n",
    "3.  Defining the CNN architecture.\n",
    "4.  A full training and validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc377bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Custom Dataset Definition ---\n",
    "class PercentageDataset(Dataset):\n",
    "    \"\"\"Custom dataset for loading percentage images.\"\"\"\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(img_dir) if f.endswith('.png')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.image_files[idx])\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"L\") # Convert to grayscale\n",
    "        \n",
    "        # Extract label from filename\n",
    "        match = re.match(r'^(\\d+)', self.image_files[idx])\n",
    "        if match:\n",
    "            label = int(match.group(1))\n",
    "        else:\n",
    "            label = -1 # Should not happen with standardized names\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# --- 2. Data Preparation ---\n",
    "# Define transforms to resize images and convert them to tensors\n",
    "# All images will be resized to 64x64 pixels\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalize for grayscale\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "full_dataset = PercentageDataset(img_dir='dataset_merged', transform=data_transforms)\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "# --- 3. CNN Model Definition ---\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        # After two pooling layers, 64x64 -> 32x32 -> 16x16\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 16 * 16) # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# --- 4. Training Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SimpleCNN(num_classes=100).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# --- 5. Training and Validation Loop ---\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "\n",
    "# --- 6. Save the Model ---\n",
    "MODEL_SAVE_PATH = \"percentage_cnn.pth\"\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
