{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb913cb",
   "metadata": {},
   "source": [
    "# Dataset Classification with Tesseract\n",
    "\n",
    "This notebook processes the images in the `dataset` folder, classifies them using Tesseract OCR, renames the files based on their classification, and provides summary metrics.\n",
    "\n",
    "**IMPORTANT:** Before running, you must have Google's Tesseract OCR engine installed on your system and accessible in your PATH. You can find installation instructions here: [https://github.com/tesseract-ocr/tesseract](https://github.com/tesseract-ocr/tesseract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pytesseract\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# If tesseract is not in your PATH, you can uncomment the following line\n",
    "# and provide the path to your tesseract.exe\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset\"\n",
    "classification_metrics = Counter()\n",
    "file_rename_counter = Counter()\n",
    "unclassified_count = 0\n",
    "\n",
    "# List all files in the dataset directory\n",
    "try:\n",
    "    image_files = [f for f in os.listdir(DATASET_DIR) if f.endswith('.png')]\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The directory '{DATASET_DIR}' was not found.\")\n",
    "    image_files = []\n",
    "\n",
    "print(f\"Found {len(image_files)} images to process.\")\n",
    "\n",
    "for filename in image_files:\n",
    "    old_filepath = os.path.join(DATASET_DIR, filename)\n",
    "    \n",
    "    # Load the image in grayscale\n",
    "    image = cv2.imread(old_filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Configure Tesseract\n",
    "    # --psm 7: Treat the image as a single text line.\n",
    "    # -c tessedit_char_whitelist: Restrict output to these characters.\n",
    "    config = \"--psm 7 -c tessedit_char_whitelist=0123456789%\"\n",
    "    \n",
    "    # Perform OCR\n",
    "    text = pytesseract.image_to_string(image, config=config).strip()\n",
    "    \n",
    "    # --- Text Cleanup and Validation ---\n",
    "    # Find the most likely percentage value\n",
    "    match = re.search(r'(\\d+)', text)\n",
    "    if match:\n",
    "        clean_text = match.group(1) + \"%\"\n",
    "    else:\n",
    "        clean_text = \"\"\n",
    "\n",
    "    # --- File Renaming ---\n",
    "    if clean_text:\n",
    "        # Update the classification metrics\n",
    "        classification_metrics[clean_text] += 1\n",
    "        \n",
    "        # Get the instance count for the new filename\n",
    "        file_rename_counter[clean_text] += 1\n",
    "        instance_count = file_rename_counter[clean_text]\n",
    "        \n",
    "        # Sanitize '%' for the filename, as it can cause issues on some systems\n",
    "        safe_label = clean_text.replace('%', 'pct')\n",
    "        new_filename = f\"{safe_label}_{instance_count}.png\"\n",
    "        new_filepath = os.path.join(DATASET_DIR, new_filename)\n",
    "        \n",
    "        try:\n",
    "            # Rename the file\n",
    "            os.rename(old_filepath, new_filepath)\n",
    "            print(f\"Renamed '{filename}' to '{new_filename}'\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error renaming file {filename}: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Could not classify '{filename}', skipping rename.\")\n",
    "        unclassified_count += 1\n",
    "\n",
    "print(\"\\n--- Processing Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907eaacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display Classification Metrics ---\n",
    "\n",
    "print(\"Tesseract Classification Metrics:\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "if classification_metrics:\n",
    "    total_classified = sum(classification_metrics.values())\n",
    "    print(f\"Total Images Classified: {total_classified}\")\n",
    "    print(f\"Total Images Unclassified: {unclassified_count}\")\n",
    "    print(f\"Number of Unique Classes: {len(classification_metrics)}\")\n",
    "    print(\"\\n--- Classification Counts ---\")\n",
    "    \n",
    "    # Sort by most common\n",
    "    for label, count in classification_metrics.most_common():\n",
    "        print(f\"- {label}: {count} times\")\n",
    "else:\n",
    "    print(\"No images were successfully classified.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0299a9dc",
   "metadata": {},
   "source": [
    "# Standardize Dataset Naming\n",
    "\n",
    "The following cell processes the images in the `dataset` folder, standardizes their filenames based on the numeric prefix, and copies them to a new `dataset_cleaned` directory. This is useful for creating a clean, consistently named dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "def rename_files_to_number(source_dir, number):\n",
    "    \"\"\"\n",
    "    Renames all files in source_dir to the format '{number}_{uniqueid}{ext}'.\n",
    "    The unique id is based on the current time and a random integer to avoid collisions.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n",
    "    for f in files:\n",
    "        ext = os.path.splitext(f)[1]\n",
    "        unique_id = f\"{int(time.time() * 1e6)}{random.randint(1000, 9999)}\"\n",
    "        new_name = f\"{number}_{unique_id}{ext}\"\n",
    "        src = os.path.join(source_dir, f)\n",
    "        dst = os.path.join(source_dir, new_name)\n",
    "        os.rename(src, dst)\n",
    "        print(f\"Renamed '{f}' -> '{new_name}'\")\n",
    "\n",
    "# Example usage:\n",
    "rename_files_to_number(\"workdir\", 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a97d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "# Define directories\n",
    "SOURCE_DIR = \"dataset\"\n",
    "DEST_DIR = \"dataset_cleaned\"\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "print(f\"Created directory: {DEST_DIR}\")\n",
    "\n",
    "# Counter for new filenames\n",
    "rename_counter = Counter()\n",
    "files_processed = 0\n",
    "files_skipped = 0\n",
    "\n",
    "# List all files in the source directory\n",
    "try:\n",
    "    image_files = [f for f in os.listdir(SOURCE_DIR) if f.endswith('.png')]\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The source directory '{SOURCE_DIR}' was not found.\")\n",
    "    image_files = []\n",
    "\n",
    "print(f\"Found {len(image_files)} images to standardize.\")\n",
    "\n",
    "# Process each file\n",
    "for filename in image_files:\n",
    "    # Extract the numeric label from the start of the filename\n",
    "    match = re.match(r'^(\\d+)', filename)\n",
    "    \n",
    "    if match:\n",
    "        label = match.group(1)\n",
    "        \n",
    "        # Remove leading zeros (convert to int and back to string)\n",
    "        label_int = int(label)\n",
    "        label_clean = str(label_int)\n",
    "        \n",
    "        # Increment the counter for this clean label\n",
    "        rename_counter[label_clean] += 1\n",
    "        instance_count = rename_counter[label_clean]\n",
    "        \n",
    "        # Create the new standardized filename with no leading zeros\n",
    "        new_filename = f\"{label_clean}pct_{instance_count}.png\"\n",
    "        \n",
    "        # Define full paths\n",
    "        old_filepath = os.path.join(SOURCE_DIR, filename)\n",
    "        new_filepath = os.path.join(DEST_DIR, new_filename)\n",
    "        \n",
    "        # Copy the file to the new directory with the new name\n",
    "        shutil.copy(old_filepath, new_filepath)\n",
    "        files_processed += 1\n",
    "    else:\n",
    "        print(f\"Could not extract label from '{filename}', skipping.\")\n",
    "        files_skipped += 1\n",
    "\n",
    "print(\"\\n--- Standardization Complete ---\")\n",
    "print(f\"Successfully processed and copied {files_processed} files.\")\n",
    "print(f\"Skipped {files_skipped} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1021d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# --- Dataset Merging Configuration ---\n",
    "# Change these folder names as needed\n",
    "FOLDER1 = \"dataset_cleaned\"  # First folder to merge\n",
    "FOLDER2 = \"dataset_cleaned2\"  # Second folder to merge - change this to your second folder\n",
    "OUTPUT_FOLDER = \"dataset_merged\"\n",
    "\n",
    "# Create the output directory\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "print(f\"Created output directory: {OUTPUT_FOLDER}\")\n",
    "\n",
    "# Counter to track instances of each label across both folders\n",
    "global_counter = Counter()\n",
    "total_files_merged = 0\n",
    "\n",
    "def merge_folder(source_folder, folder_name):\n",
    "    \"\"\"\n",
    "    Merge files from a source folder into the output folder.\n",
    "    \n",
    "    Args:\n",
    "        source_folder: Path to the source folder\n",
    "        folder_name: Name of the folder (for logging purposes)\n",
    "    \"\"\"\n",
    "    global global_counter, total_files_merged\n",
    "    \n",
    "    if not os.path.exists(source_folder):\n",
    "        print(f\"Warning: {source_folder} does not exist, skipping.\")\n",
    "        return\n",
    "    \n",
    "    files_in_folder = [f for f in os.listdir(source_folder) if f.endswith('.png')]\n",
    "    print(f\"\\nProcessing {len(files_in_folder)} files from {folder_name}...\")\n",
    "    \n",
    "    for filename in files_in_folder:\n",
    "        # Extract the numeric label from the filename\n",
    "        match = re.match(r'^(\\d+)', filename)\n",
    "        \n",
    "        if match:\n",
    "            label = match.group(1)\n",
    "            \n",
    "            # Remove leading zeros (convert to int and back to string)\n",
    "            label_int = int(label)\n",
    "            label_clean = str(label_int)\n",
    "            \n",
    "            # Increment the global counter for this clean label\n",
    "            global_counter[label_clean] += 1\n",
    "            instance_count = global_counter[label_clean]\n",
    "            \n",
    "            # Create the new standardized filename with no leading zeros\n",
    "            new_filename = f\"{label_clean}pct_{instance_count}.png\"\n",
    "            \n",
    "            # Define full paths\n",
    "            old_filepath = os.path.join(source_folder, filename)\n",
    "            new_filepath = os.path.join(OUTPUT_FOLDER, new_filename)\n",
    "            \n",
    "            # Copy the file with the new name\n",
    "            shutil.copy(old_filepath, new_filepath)\n",
    "            total_files_merged += 1\n",
    "            \n",
    "            if total_files_merged % 100 == 0:  # Progress update every 100 files\n",
    "                print(f\"Merged {total_files_merged} files...\")\n",
    "        else:\n",
    "            print(f\"Could not extract label from '{filename}', skipping.\")\n",
    "\n",
    "# Merge both folders\n",
    "print(\"=\"*50)\n",
    "print(\"MERGING DATASET FOLDERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "merge_folder(FOLDER1, \"Folder 1 (dataset_cleaned)\")\n",
    "merge_folder(FOLDER2, \"Folder 2\")\n",
    "\n",
    "print(f\"\\n--- Merging Complete ---\")\n",
    "print(f\"Total files merged: {total_files_merged}\")\n",
    "print(f\"Output directory: {OUTPUT_FOLDER}\")\n",
    "print(f\"Number of unique labels: {len(global_counter)}\")\n",
    "\n",
    "# Display label distribution\n",
    "print(\"\\n--- Label Distribution ---\")\n",
    "for label, count in sorted(global_counter.items(), key=lambda x: int(x[0])):\n",
    "    print(f\"Label {label}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Source and destination directories\n",
    "SRC_DIR = OUTPUT_FOLDER  # 'dataset_merged'\n",
    "DEST_DIR = \"dataset_ends\"\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "\n",
    "# Find all png files and group by label\n",
    "label_to_files = defaultdict(list)\n",
    "for fname in os.listdir(SRC_DIR):\n",
    "    if fname.endswith('.png'):\n",
    "        match = re.match(r'^(\\d+)pct_(\\d+)\\.png$', fname)\n",
    "        if match:\n",
    "            label = match.group(1)\n",
    "            counter = int(match.group(2))\n",
    "            label_to_files[label].append((counter, fname))\n",
    "\n",
    "# For each label, find min and max counter and copy those files\n",
    "files_copied = 0\n",
    "for label, files in label_to_files.items():\n",
    "    if not files:\n",
    "        continue\n",
    "    # Sort by counter\n",
    "    files_sorted = sorted(files, key=lambda x: x[0])\n",
    "    min_counter, min_file = files_sorted[0]\n",
    "    max_counter, max_file = files_sorted[-1]\n",
    "    # Copy min\n",
    "    src_min = os.path.join(SRC_DIR, min_file)\n",
    "    dst_min = os.path.join(DEST_DIR, min_file)\n",
    "    shutil.copy(src_min, dst_min)\n",
    "    files_copied += 1\n",
    "    # If min and max are different, copy max\n",
    "    if min_file != max_file:\n",
    "        src_max = os.path.join(SRC_DIR, max_file)\n",
    "        dst_max = os.path.join(DEST_DIR, max_file)\n",
    "        shutil.copy(src_max, dst_max)\n",
    "        files_copied += 1\n",
    "\n",
    "print(f\"Copied {files_copied} files to {DEST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02a58c",
   "metadata": {},
   "source": [
    "# CNN Training with PyTorch\n",
    "\n",
    "This section defines and trains a Convolutional Neural Network (CNN) on the cleaned dataset.\n",
    "The process includes:\n",
    "1.  A custom `Dataset` class to load images and parse labels from filenames.\n",
    "2.  Splitting the data into training and validation sets.\n",
    "3.  Defining the CNN architecture.\n",
    "4.  A full training and validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6ff589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Model Configuration: Default model type set to 'optimized'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# Import our optimized CNN architecture\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "from models import get_model, count_parameters, benchmark_model, get_default_model_type, get_model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ca4480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Architecture Comparison\n",
      "==================================================\n",
      "Current default model type: 'optimized'\n",
      "To change: Edit DEFAULT_MODEL_TYPE in src/models/percentage_cnn.py\n",
      "\n",
      "ðŸ‘‰ OPTIMIZED CNN:\n",
      "     Description: Best balance of accuracy and speed\n",
      "     Parameters: 688,392\n",
      "     Expected speed: Fast (~2-4ms)\n",
      "     Actual inference time: 0.99ms\n",
      "     âœ… Within target (<5ms)\n",
      "\n",
      "   LIGHTWEIGHT CNN:\n",
      "     Description: Maximum speed for real-time applications\n",
      "     Parameters: 167,508\n",
      "     Expected speed: Very fast (~1-2ms)\n",
      "     Actual inference time: 0.58ms\n",
      "     âœ… Within target (<5ms)\n",
      "\n",
      "   SIMPLE CNN:\n",
      "     Description: Original architecture for compatibility\n",
      "     Parameters: 4,250,916\n",
      "     Expected speed: Moderate (~3-5ms)\n",
      "     Actual inference time: 0.36ms\n",
      "     âœ… Within target (<5ms)\n",
      "\n",
      "To change default model:\n",
      "  1. Edit DEFAULT_MODEL_TYPE in src/models/percentage_cnn.py\n",
      "  2. Options: 'optimized', 'lightweight', 'simple'\n",
      "  3. Restart your notebook/script to apply changes\n",
      "     Actual inference time: 0.99ms\n",
      "     âœ… Within target (<5ms)\n",
      "\n",
      "   LIGHTWEIGHT CNN:\n",
      "     Description: Maximum speed for real-time applications\n",
      "     Parameters: 167,508\n",
      "     Expected speed: Very fast (~1-2ms)\n",
      "     Actual inference time: 0.58ms\n",
      "     âœ… Within target (<5ms)\n",
      "\n",
      "   SIMPLE CNN:\n",
      "     Description: Original architecture for compatibility\n",
      "     Parameters: 4,250,916\n",
      "     Expected speed: Moderate (~3-5ms)\n",
      "     Actual inference time: 0.36ms\n",
      "     âœ… Within target (<5ms)\n",
      "\n",
      "To change default model:\n",
      "  1. Edit DEFAULT_MODEL_TYPE in src/models/percentage_cnn.py\n",
      "  2. Options: 'optimized', 'lightweight', 'simple'\n",
      "  3. Restart your notebook/script to apply changes\n"
     ]
    }
   ],
   "source": [
    "# --- Compare CNN Architectures ---\n",
    "print(\"CNN Architecture Comparison\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show current default\n",
    "current_default = get_default_model_type()\n",
    "print(f\"Current default model type: '{current_default}'\")\n",
    "print(f\"To change: Edit DEFAULT_MODEL_TYPE in src/models/percentage_cnn.py\")\n",
    "\n",
    "# Compare different model architectures\n",
    "architectures = [\"optimized\", \"lightweight\", \"simple\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for arch in architectures:\n",
    "    model_test = get_model(arch, num_classes=100)\n",
    "    params = count_parameters(model_test)\n",
    "    info = get_model_info(arch)\n",
    "    \n",
    "    marker = \"ðŸ‘‰\" if arch == current_default else \"  \"\n",
    "    print(f\"\\n{marker} {arch.upper()} CNN:\")\n",
    "    print(f\"     Description: {info['description']}\")\n",
    "    print(f\"     Parameters: {params:,}\")\n",
    "    print(f\"     Expected speed: {info['speed']}\")\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        try:\n",
    "            inference_time = benchmark_model(model_test, device=device, num_runs=50)\n",
    "            print(f\"     Actual inference time: {inference_time:.2f}ms\")\n",
    "            \n",
    "            if inference_time <= 5.0:\n",
    "                print(f\"     âœ… Within target (<5ms)\")\n",
    "            else:\n",
    "                print(f\"     âš ï¸  Exceeds target (>5ms)\")\n",
    "        except Exception as e:\n",
    "            print(f\"     Benchmark failed: {e}\")\n",
    "    else:\n",
    "        print(f\"     Benchmark skipped (CPU mode)\")\n",
    "\n",
    "print(f\"\\nTo change default model:\")\n",
    "print(f\"  1. Edit DEFAULT_MODEL_TYPE in src/models/percentage_cnn.py\")\n",
    "print(f\"  2. Options: 'optimized', 'lightweight', 'simple'\")\n",
    "print(f\"  3. Restart your notebook/script to apply changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc377bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 17096\n",
      "Validation set size: 4274\n",
      "ðŸ“‹ Using default model type: 'optimized'\n",
      "Using optimized CNN architecture (configured in src/models/percentage_cnn.py)\n",
      "Model parameters: 688,392\n",
      "Model info: Best balance of accuracy and speed\n",
      "Use case: Recommended for most applications\n",
      "Expected speed: Fast (~2-4ms)\n",
      "Actual inference time: 1.23ms\n",
      "âœ… Inference time within target (<5ms)\n",
      "Training set size: 17096\n",
      "Validation set size: 4274\n",
      "Using device: cuda\n",
      "Actual inference time: 1.23ms\n",
      "âœ… Inference time within target (<5ms)\n",
      "Training set size: 17096\n",
      "Validation set size: 4274\n",
      "Using device: cuda\n",
      "Epoch [1/15], Train Loss: 3.5228, Val Loss: 0.0103, Val Accuracy: 90.99%\n",
      "  Learning rate: 0.001000\n",
      "  Failed predictions this epoch: 385\n",
      "    55pct_20_diag1.png: true=55, pred=53, conf=0.479\n",
      "    58pct_21_trans1.png: true=58, pred=88, conf=0.564\n",
      "    45pct_13_scale1.png: true=45, pred=64, conf=0.285\n",
      "    52pct_44_trans1.png: true=52, pred=25, conf=0.255\n",
      "    45pct_29_trans1.png: true=45, pred=54, conf=0.509\n",
      "    ... and 380 more\n",
      "    Debug - First fail: val_idx=0, orig_idx=10924\n",
      "\n",
      "Epoch [1/15], Train Loss: 3.5228, Val Loss: 0.0103, Val Accuracy: 90.99%\n",
      "  Learning rate: 0.001000\n",
      "  Failed predictions this epoch: 385\n",
      "    55pct_20_diag1.png: true=55, pred=53, conf=0.479\n",
      "    58pct_21_trans1.png: true=58, pred=88, conf=0.564\n",
      "    45pct_13_scale1.png: true=45, pred=64, conf=0.285\n",
      "    52pct_44_trans1.png: true=52, pred=25, conf=0.255\n",
      "    45pct_29_trans1.png: true=45, pred=54, conf=0.509\n",
      "    ... and 380 more\n",
      "    Debug - First fail: val_idx=0, orig_idx=10924\n",
      "\n",
      "Epoch [2/15], Train Loss: 0.4181, Val Loss: 0.0029, Val Accuracy: 97.96%\n",
      "  Learning rate: 0.001000\n",
      "  Failed predictions this epoch: 87\n",
      "    0pct_149_trans1.png: true=0, pred=11, conf=0.793\n",
      "    96pct_39.png: true=96, pred=36, conf=0.750\n",
      "    75pct_59_trans1.png: true=75, pred=36, conf=0.681\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.825\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.975\n",
      "    ... and 82 more\n",
      "    Debug - First fail: val_idx=6, orig_idx=136\n",
      "\n",
      "Epoch [2/15], Train Loss: 0.4181, Val Loss: 0.0029, Val Accuracy: 97.96%\n",
      "  Learning rate: 0.001000\n",
      "  Failed predictions this epoch: 87\n",
      "    0pct_149_trans1.png: true=0, pred=11, conf=0.793\n",
      "    96pct_39.png: true=96, pred=36, conf=0.750\n",
      "    75pct_59_trans1.png: true=75, pred=36, conf=0.681\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.825\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.975\n",
      "    ... and 82 more\n",
      "    Debug - First fail: val_idx=6, orig_idx=136\n",
      "\n",
      "Epoch [3/15], Train Loss: 0.1784, Val Loss: 0.0065, Val Accuracy: 94.67%\n",
      "  Learning rate: 0.001000\n",
      "  Failed predictions this epoch: 228\n",
      "    93pct_25_trans1.png: true=93, pred=39, conf=0.551\n",
      "    28pct_16_scale1.png: true=28, pred=29, conf=0.623\n",
      "    53pct_16.png: true=53, pred=33, conf=0.551\n",
      "    28pct_39_trans1.png: true=28, pred=29, conf=0.983\n",
      "    95pct_15_trans1.png: true=95, pred=29, conf=0.578\n",
      "    ... and 223 more\n",
      "    Debug - First fail: val_idx=1, orig_idx=19974\n",
      "\n",
      "Epoch [3/15], Train Loss: 0.1784, Val Loss: 0.0065, Val Accuracy: 94.67%\n",
      "  Learning rate: 0.001000\n",
      "  Failed predictions this epoch: 228\n",
      "    93pct_25_trans1.png: true=93, pred=39, conf=0.551\n",
      "    28pct_16_scale1.png: true=28, pred=29, conf=0.623\n",
      "    53pct_16.png: true=53, pred=33, conf=0.551\n",
      "    28pct_39_trans1.png: true=28, pred=29, conf=0.983\n",
      "    95pct_15_trans1.png: true=95, pred=29, conf=0.578\n",
      "    ... and 223 more\n",
      "    Debug - First fail: val_idx=1, orig_idx=19974\n",
      "\n",
      "Epoch [4/15], Train Loss: 0.1303, Val Loss: 0.0067, Val Accuracy: 93.17%\n",
      "  Learning rate: 0.001000\n",
      "  Failed predictions this epoch: 292\n",
      "    93pct_25_trans1.png: true=93, pred=33, conf=0.963\n",
      "    30pct_12_trans1.png: true=30, pred=36, conf=0.461\n",
      "    20pct_63_trans1.png: true=20, pred=24, conf=0.472\n",
      "    47pct_46_diag1.png: true=47, pred=7, conf=0.632\n",
      "    30pct_15.png: true=30, pred=38, conf=0.788\n",
      "    ... and 287 more\n",
      "    Debug - First fail: val_idx=1, orig_idx=19974\n",
      "\n",
      "Epoch [4/15], Train Loss: 0.1303, Val Loss: 0.0067, Val Accuracy: 93.17%\n",
      "  Learning rate: 0.001000\n",
      "  Failed predictions this epoch: 292\n",
      "    93pct_25_trans1.png: true=93, pred=33, conf=0.963\n",
      "    30pct_12_trans1.png: true=30, pred=36, conf=0.461\n",
      "    20pct_63_trans1.png: true=20, pred=24, conf=0.472\n",
      "    47pct_46_diag1.png: true=47, pred=7, conf=0.632\n",
      "    30pct_15.png: true=30, pred=38, conf=0.788\n",
      "    ... and 287 more\n",
      "    Debug - First fail: val_idx=1, orig_idx=19974\n",
      "\n",
      "Epoch [5/15], Train Loss: 0.1180, Val Loss: 0.0019, Val Accuracy: 98.92%\n",
      "  Learning rate: 0.000500\n",
      "  Failed predictions this epoch: 46\n",
      "    75pct_59_trans1.png: true=75, pred=25, conf=0.982\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.999\n",
      "    5pct_38_trans1.png: true=5, pred=55, conf=0.731\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.902\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.881\n",
      "    ... and 41 more\n",
      "    Debug - First fail: val_idx=187, orig_idx=15839\n",
      "\n",
      "Epoch [5/15], Train Loss: 0.1180, Val Loss: 0.0019, Val Accuracy: 98.92%\n",
      "  Learning rate: 0.000500\n",
      "  Failed predictions this epoch: 46\n",
      "    75pct_59_trans1.png: true=75, pred=25, conf=0.982\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.999\n",
      "    5pct_38_trans1.png: true=5, pred=55, conf=0.731\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.902\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.881\n",
      "    ... and 41 more\n",
      "    Debug - First fail: val_idx=187, orig_idx=15839\n",
      "\n",
      "Epoch [6/15], Train Loss: 0.0541, Val Loss: 0.0013, Val Accuracy: 99.25%\n",
      "  Learning rate: 0.000500\n",
      "  Failed predictions this epoch: 32\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.987\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.997\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.878\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.494\n",
      "    60pct_35_trans1.png: true=60, pred=50, conf=0.958\n",
      "    ... and 27 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [6/15], Train Loss: 0.0541, Val Loss: 0.0013, Val Accuracy: 99.25%\n",
      "  Learning rate: 0.000500\n",
      "  Failed predictions this epoch: 32\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.987\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.997\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.878\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.494\n",
      "    60pct_35_trans1.png: true=60, pred=50, conf=0.958\n",
      "    ... and 27 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [7/15], Train Loss: 0.0437, Val Loss: 0.0012, Val Accuracy: 99.13%\n",
      "  Learning rate: 0.000500\n",
      "  Failed predictions this epoch: 37\n",
      "    95pct_15_trans1.png: true=95, pred=85, conf=0.615\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.860\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.633\n",
      "    54pct_27_trans1.png: true=54, pred=64, conf=0.728\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.967\n",
      "    ... and 32 more\n",
      "    Debug - First fail: val_idx=96, orig_idx=20335\n",
      "\n",
      "Epoch [7/15], Train Loss: 0.0437, Val Loss: 0.0012, Val Accuracy: 99.13%\n",
      "  Learning rate: 0.000500\n",
      "  Failed predictions this epoch: 37\n",
      "    95pct_15_trans1.png: true=95, pred=85, conf=0.615\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.860\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.633\n",
      "    54pct_27_trans1.png: true=54, pred=64, conf=0.728\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.967\n",
      "    ... and 32 more\n",
      "    Debug - First fail: val_idx=96, orig_idx=20335\n",
      "\n",
      "Epoch [8/15], Train Loss: 0.0375, Val Loss: 0.0012, Val Accuracy: 99.13%\n",
      "  Learning rate: 0.000500\n",
      "  Failed predictions this epoch: 37\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.922\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.977\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.988\n",
      "    49pct_48_trans1.png: true=49, pred=69, conf=0.569\n",
      "    51pct_43_diag1.png: true=51, pred=5, conf=0.899\n",
      "    ... and 32 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [8/15], Train Loss: 0.0375, Val Loss: 0.0012, Val Accuracy: 99.13%\n",
      "  Learning rate: 0.000500\n",
      "  Failed predictions this epoch: 37\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.922\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.977\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.988\n",
      "    49pct_48_trans1.png: true=49, pred=69, conf=0.569\n",
      "    51pct_43_diag1.png: true=51, pred=5, conf=0.899\n",
      "    ... and 32 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [9/15], Train Loss: 0.0403, Val Loss: 0.0012, Val Accuracy: 99.09%\n",
      "  Learning rate: 0.000500\n",
      "  Failed predictions this epoch: 39\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.979\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.962\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.660\n",
      "    49pct_48_trans1.png: true=49, pred=69, conf=0.495\n",
      "    57pct_33_scale1.png: true=57, pred=97, conf=0.538\n",
      "    ... and 34 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [9/15], Train Loss: 0.0403, Val Loss: 0.0012, Val Accuracy: 99.09%\n",
      "  Learning rate: 0.000500\n",
      "  Failed predictions this epoch: 39\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.979\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.962\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.660\n",
      "    49pct_48_trans1.png: true=49, pred=69, conf=0.495\n",
      "    57pct_33_scale1.png: true=57, pred=97, conf=0.538\n",
      "    ... and 34 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [10/15], Train Loss: 0.0514, Val Loss: 0.0014, Val Accuracy: 99.09%\n",
      "  Learning rate: 0.000250\n",
      "  Failed predictions this epoch: 39\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.917\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.920\n",
      "    56pct_54_trans1.png: true=56, pred=66, conf=0.813\n",
      "    49pct_48_trans1.png: true=49, pred=59, conf=0.352\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.938\n",
      "    ... and 34 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [10/15], Train Loss: 0.0514, Val Loss: 0.0014, Val Accuracy: 99.09%\n",
      "  Learning rate: 0.000250\n",
      "  Failed predictions this epoch: 39\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.917\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.920\n",
      "    56pct_54_trans1.png: true=56, pred=66, conf=0.813\n",
      "    49pct_48_trans1.png: true=49, pred=59, conf=0.352\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.938\n",
      "    ... and 34 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [11/15], Train Loss: 0.0280, Val Loss: 0.0010, Val Accuracy: 99.27%\n",
      "  Learning rate: 0.000250\n",
      "  Failed predictions this epoch: 31\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.966\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.996\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.980\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.983\n",
      "    60pct_35_trans1.png: true=60, pred=50, conf=0.929\n",
      "    ... and 26 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [11/15], Train Loss: 0.0280, Val Loss: 0.0010, Val Accuracy: 99.27%\n",
      "  Learning rate: 0.000250\n",
      "  Failed predictions this epoch: 31\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.966\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.996\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.980\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.983\n",
      "    60pct_35_trans1.png: true=60, pred=50, conf=0.929\n",
      "    ... and 26 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [12/15], Train Loss: 0.0224, Val Loss: 0.0009, Val Accuracy: 99.39%\n",
      "  Learning rate: 0.000250\n",
      "  Failed predictions this epoch: 26\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.765\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.980\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.944\n",
      "    49pct_48_trans1.png: true=49, pred=59, conf=0.575\n",
      "    69pct_24.png: true=69, pred=59, conf=0.507\n",
      "    ... and 21 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [12/15], Train Loss: 0.0224, Val Loss: 0.0009, Val Accuracy: 99.39%\n",
      "  Learning rate: 0.000250\n",
      "  Failed predictions this epoch: 26\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.765\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.980\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.944\n",
      "    49pct_48_trans1.png: true=49, pred=59, conf=0.575\n",
      "    69pct_24.png: true=69, pred=59, conf=0.507\n",
      "    ... and 21 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [13/15], Train Loss: 0.0167, Val Loss: 0.0009, Val Accuracy: 99.34%\n",
      "  Learning rate: 0.000250\n",
      "  Failed predictions this epoch: 28\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.929\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.968\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.990\n",
      "    54pct_27_trans1.png: true=54, pred=64, conf=0.547\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.836\n",
      "    ... and 23 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [13/15], Train Loss: 0.0167, Val Loss: 0.0009, Val Accuracy: 99.34%\n",
      "  Learning rate: 0.000250\n",
      "  Failed predictions this epoch: 28\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.929\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.968\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.990\n",
      "    54pct_27_trans1.png: true=54, pred=64, conf=0.547\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.836\n",
      "    ... and 23 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [14/15], Train Loss: 0.0197, Val Loss: 0.0010, Val Accuracy: 99.30%\n",
      "  Learning rate: 0.000250\n",
      "  Failed predictions this epoch: 30\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.787\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.967\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.973\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.996\n",
      "    22pct_73_trans1.png: true=22, pred=72, conf=0.955\n",
      "    ... and 25 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [14/15], Train Loss: 0.0197, Val Loss: 0.0010, Val Accuracy: 99.30%\n",
      "  Learning rate: 0.000250\n",
      "  Failed predictions this epoch: 30\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=0.787\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=0.967\n",
      "    56pct_54_trans1.png: true=56, pred=6, conf=0.973\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.996\n",
      "    22pct_73_trans1.png: true=22, pred=72, conf=0.955\n",
      "    ... and 25 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "Epoch [15/15], Train Loss: 0.0211, Val Loss: 0.0011, Val Accuracy: 99.39%\n",
      "  Learning rate: 0.000125\n",
      "  Failed predictions this epoch: 26\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=1.000\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=1.000\n",
      "    49pct_48_trans1.png: true=49, pred=59, conf=0.549\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.997\n",
      "    60pct_35_trans1.png: true=60, pred=50, conf=0.962\n",
      "    ... and 21 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "--- Copying Failed Validation Files ---\n",
      "Total unique files that failed validation: 751\n",
      "Epoch [15/15], Train Loss: 0.0211, Val Loss: 0.0011, Val Accuracy: 99.39%\n",
      "  Learning rate: 0.000125\n",
      "  Failed predictions this epoch: 26\n",
      "    62pct_53_trans1.png: true=62, pred=52, conf=1.000\n",
      "    60pct_34_trans1.png: true=60, pred=50, conf=1.000\n",
      "    49pct_48_trans1.png: true=49, pred=59, conf=0.549\n",
      "    88pct_37_trans1.png: true=88, pred=38, conf=0.997\n",
      "    60pct_35_trans1.png: true=60, pred=50, conf=0.962\n",
      "    ... and 21 more\n",
      "    Debug - First fail: val_idx=196, orig_idx=12666\n",
      "\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "--- Copying Failed Validation Files ---\n",
      "Total unique files that failed validation: 751\n",
      "Successfully copied 751 failed validation files to 'dataset_failed'\n",
      "Verification: 'dataset_failed' now contains 3151 PNG files\n",
      "Model saved to percentage_cnn_optimized.pth\n",
      "Model info saved to percentage_cnn_optimized_info.json\n",
      "Successfully copied 751 failed validation files to 'dataset_failed'\n",
      "Verification: 'dataset_failed' now contains 3151 PNG files\n",
      "Model saved to percentage_cnn_optimized.pth\n",
      "Model info saved to percentage_cnn_optimized_info.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 1. Custom Dataset Definition ---\n",
    "class PercentageDataset(Dataset):\n",
    "    \"\"\"Custom dataset for loading percentage images.\"\"\"\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(img_dir) if f.endswith('.png')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.image_files[idx])\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"L\") # Convert to grayscale\n",
    "        \n",
    "        # Extract label from filename\n",
    "        match = re.match(r'^(\\d+)', self.image_files[idx])\n",
    "        if match:\n",
    "            label = int(match.group(1))\n",
    "        else:\n",
    "            label = -1 # Should not happen with standardized names\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# --- 2. Data Preparation ---\n",
    "# Define transforms to resize images and convert them to tensors\n",
    "# All images will be resized to 64x64 pixels\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalize for grayscale\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "full_dataset = PercentageDataset(img_dir='dataset_final', transform=data_transforms)\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "# --- 3. CNN Model Definition ---\n",
    "# Use the configured default CNN architecture\n",
    "model_type = get_default_model_type()  # Gets the model type set in src/models/percentage_cnn.py\n",
    "model = get_model(num_classes=100).to(device)  # Uses default model type\n",
    "\n",
    "print(f\"Using {model_type} CNN architecture (configured in src/models/percentage_cnn.py)\")\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# Show model information\n",
    "model_info = get_model_info()\n",
    "print(f\"Model info: {model_info['description']}\")\n",
    "print(f\"Use case: {model_info['use_case']}\")\n",
    "print(f\"Expected speed: {model_info['speed']}\")\n",
    "\n",
    "# Benchmark the model if on GPU\n",
    "if device.type == 'cuda':\n",
    "    inference_time = benchmark_model(model, device=device, num_runs=50)\n",
    "    print(f\"Actual inference time: {inference_time:.2f}ms\")\n",
    "    if inference_time > 5.0:\n",
    "        print(\"âš ï¸  Warning: Inference time > 5ms. Consider changing DEFAULT_MODEL_TYPE to 'lightweight' in src/models/percentage_cnn.py\")\n",
    "    else:\n",
    "        print(\"âœ… Inference time within target (<5ms)\")\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "# --- 4. Training Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Added weight decay for regularization\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Learning rate scheduler\n",
    "num_epochs = 15  # Increased epochs for better convergence with new architecture\n",
    "\n",
    "# --- 5. Training and Validation Loop ---\n",
    "# Create directory for failed validation files\n",
    "FAILED_DIR = \"dataset_failed\"\n",
    "os.makedirs(FAILED_DIR, exist_ok=True)\n",
    "\n",
    "# Track failed files across all epochs\n",
    "all_failed_files = set()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation with detailed failure tracking\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    failed_files_this_epoch = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process validation in batches for efficiency, but track individual files\n",
    "        for batch_idx, (images, labels) in enumerate(val_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Make predictions for the batch\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Process each item in the batch to track individual files\n",
    "            for i in range(len(images)):\n",
    "                # Calculate the original dataset index for this validation item\n",
    "                val_idx = batch_idx * val_loader.batch_size + i\n",
    "                if val_idx >= len(val_dataset):  # Handle last batch edge case\n",
    "                    break\n",
    "                    \n",
    "                original_idx = val_dataset.indices[val_idx]\n",
    "                filename = full_dataset.image_files[original_idx]\n",
    "                \n",
    "                true_label = labels[i].item()\n",
    "                pred_label = predicted[i].item()\n",
    "                confidence = torch.softmax(outputs[i].unsqueeze(0), 1)[0][pred_label].item()\n",
    "                \n",
    "                total += 1\n",
    "                \n",
    "                if pred_label == true_label:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    # Track failed prediction with detailed info\n",
    "                    failed_info = {\n",
    "                        'filename': filename,\n",
    "                        'true_label': true_label,\n",
    "                        'predicted_label': pred_label,\n",
    "                        'confidence': confidence,\n",
    "                        'val_idx': val_idx,\n",
    "                        'original_idx': original_idx\n",
    "                    }\n",
    "                    failed_files_this_epoch.append(failed_info)\n",
    "                    all_failed_files.add(filename)\n",
    "            \n",
    "    val_loss /= len(val_dataset)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"  Learning rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # Show failed files for this epoch (limit to first 5 for readability)\n",
    "    if failed_files_this_epoch:\n",
    "        print(f\"  Failed predictions this epoch: {len(failed_files_this_epoch)}\")\n",
    "        for i, fail_info in enumerate(failed_files_this_epoch[:5]):\n",
    "            print(f\"    {fail_info['filename']}: true={fail_info['true_label']}, \"\n",
    "                  f\"pred={fail_info['predicted_label']}, conf={fail_info['confidence']:.3f}\")\n",
    "        if len(failed_files_this_epoch) > 5:\n",
    "            print(f\"    ... and {len(failed_files_this_epoch) - 5} more\")\n",
    "        \n",
    "        # Debug: Show first failed file's indices to verify correctness\n",
    "        if len(failed_files_this_epoch) > 0:\n",
    "            first_fail = failed_files_this_epoch[0]\n",
    "            print(f\"    Debug - First fail: val_idx={first_fail['val_idx']}, \"\n",
    "                  f\"orig_idx={first_fail['original_idx']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "\n",
    "# --- Copy Failed Files to Analysis Folder ---\n",
    "print(f\"\\n--- Copying Failed Validation Files ---\")\n",
    "print(f\"Total unique files that failed validation: {len(all_failed_files)}\")\n",
    "\n",
    "failed_copied = 0\n",
    "failed_not_found = 0\n",
    "for filename in sorted(all_failed_files):  # Sort for consistent processing\n",
    "    src_path = os.path.join(full_dataset.img_dir, filename)\n",
    "    dst_path = os.path.join(FAILED_DIR, filename)\n",
    "    \n",
    "    if os.path.exists(src_path):\n",
    "        try:\n",
    "            shutil.copy(src_path, dst_path)\n",
    "            failed_copied += 1\n",
    "            # Verify the copy worked\n",
    "            if not os.path.exists(dst_path):\n",
    "                print(f\"Warning: Copy verification failed for {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {filename}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: Source file not found: {src_path}\")\n",
    "        failed_not_found += 1\n",
    "\n",
    "print(f\"Successfully copied {failed_copied} failed validation files to '{FAILED_DIR}'\")\n",
    "if failed_not_found > 0:\n",
    "    print(f\"Warning: {failed_not_found} files were not found in source directory\")\n",
    "\n",
    "# Verify the failed directory contents\n",
    "copied_files = [f for f in os.listdir(FAILED_DIR) if f.endswith('.png')]\n",
    "print(f\"Verification: '{FAILED_DIR}' now contains {len(copied_files)} PNG files\")\n",
    "\n",
    "# --- 6. Save the Model ---\n",
    "MODEL_SAVE_PATH = f\"percentage_cnn_{model_type}.pth\"\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Also save model info for easy loading\n",
    "model_info = {\n",
    "    'model_type': model_type,\n",
    "    'num_classes': 100,\n",
    "    'input_size': (64, 64),\n",
    "    'parameters': count_parameters(model)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"percentage_cnn_{model_type}_info.json\", 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "print(f\"Model info saved to percentage_cnn_{model_type}_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6811d48",
   "metadata": {},
   "source": [
    "# Model Evaluation on Original Dataset\n",
    "\n",
    "This section evaluates the trained model's performance on the original `dataset_merged` dataset to see how well it generalizes to the base data without amplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45503e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATING MODEL ON ORIGINAL DATASET (dataset_merged)\n",
      "============================================================\n",
      "Original dataset size: 6543 images\n",
      "\n",
      "--- Overall Performance ---\n",
      "Total Images: 6543\n",
      "Correct Predictions: 6535\n",
      "Overall Accuracy: 99.88%\n",
      "Average Loss: 0.0073\n",
      "Error Rate: 0.12%\n",
      "\n",
      "--- Per-Percentage Performance ---\n",
      "Label | Accuracy | Count | Avg Confidence\n",
      "---------------------------------------------\n",
      "  0%  |  100.0%  |  173  | 1.000\n",
      "  1%  |  100.0%  |   81  | 1.000\n",
      "  2%  |  100.0%  |   76  | 1.000\n",
      "  3%  |  100.0%  |   71  | 1.000\n",
      "  4%  |  100.0%  |   71  | 1.000\n",
      "  5%  |   97.3%  |   75  | 1.000\n",
      "  6%  |  100.0%  |   76  | 1.000\n",
      "  7%  |  100.0%  |   86  | 1.000\n",
      "  8%  |  100.0%  |   76  | 1.000\n",
      "  9%  |  100.0%  |   72  | 1.000\n",
      " 10%  |  100.0%  |   78  | 1.000\n",
      " 11%  |  100.0%  |   73  | 1.000\n",
      " 12%  |  100.0%  |   77  | 1.000\n",
      " 13%  |  100.0%  |   69  | 1.000\n",
      " 14%  |  100.0%  |   72  | 1.000\n",
      " 15%  |  100.0%  |   64  | 1.000\n",
      " 16%  |  100.0%  |   64  | 1.000\n",
      " 17%  |  100.0%  |   61  | 1.000\n",
      " 18%  |  100.0%  |   56  | 1.000\n",
      " 19%  |  100.0%  |   64  | 1.000\n",
      " 20%  |  100.0%  |   68  | 1.000\n",
      " 21%  |  100.0%  |   71  | 1.000\n",
      " 22%  |  100.0%  |   80  | 1.000\n",
      " 23%  |  100.0%  |   71  | 1.000\n",
      " 24%  |  100.0%  |   68  | 1.000\n",
      " 25%  |  100.0%  |   66  | 1.000\n",
      " 26%  |  100.0%  |   59  | 1.000\n",
      " 27%  |  100.0%  |   62  | 1.000\n",
      " 28%  |  100.0%  |   59  | 1.000\n",
      " 29%  |  100.0%  |   56  | 1.000\n",
      " 30%  |  100.0%  |   57  | 1.000\n",
      " 31%  |  100.0%  |   75  | 1.000\n",
      " 32%  |  100.0%  |   53  | 1.000\n",
      " 33%  |  100.0%  |   61  | 1.000\n",
      " 34%  |  100.0%  |   57  | 1.000\n",
      " 35%  |  100.0%  |   62  | 1.000\n",
      " 36%  |  100.0%  |   63  | 1.000\n",
      " 37%  |  100.0%  |   69  | 1.000\n",
      " 38%  |  100.0%  |   67  | 1.000\n",
      " 39%  |  100.0%  |   72  | 1.000\n",
      " 40%  |  100.0%  |   86  | 1.000\n",
      " 41%  |  100.0%  |   71  | 1.000\n",
      " 42%  |  100.0%  |   73  | 1.000\n",
      " 43%  |  100.0%  |   51  | 1.000\n",
      " 44%  |  100.0%  |   59  | 1.000\n",
      " 45%  |  100.0%  |   54  | 1.000\n",
      " 46%  |  100.0%  |   60  | 1.000\n",
      " 47%  |  100.0%  |   59  | 1.000\n",
      " 48%  |  100.0%  |   63  | 1.000\n",
      " 49%  |  100.0%  |   63  | 1.000\n",
      " 50%  |  100.0%  |   66  | 1.000\n",
      " 51%  |  100.0%  |   71  | 1.000\n",
      " 52%  |  100.0%  |   72  | 1.000\n",
      " 53%  |  100.0%  |   54  | 1.000\n",
      " 54%  |  100.0%  |   72  | 1.000\n",
      " 55%  |  100.0%  |   50  | 0.999\n",
      " 56%  |  100.0%  |   61  | 1.000\n",
      " 57%  |  100.0%  |   50  | 1.000\n",
      " 58%  |  100.0%  |   67  | 1.000\n",
      " 59%  |  100.0%  |   66  | 0.997\n",
      " 60%  |  100.0%  |   64  | 1.000\n",
      " 61%  |  100.0%  |   60  | 1.000\n",
      " 62%  |  100.0%  |   64  | 1.000\n",
      " 63%  |  100.0%  |  115  | 1.000\n",
      " 64%  |  100.0%  |   60  | 1.000\n",
      " 65%  |  100.0%  |   59  | 1.000\n",
      " 66%  |  100.0%  |   62  | 1.000\n",
      " 67%  |  100.0%  |   63  | 1.000\n",
      " 68%  |  100.0%  |   72  | 1.000\n",
      " 69%  |   95.3%  |   43  | 0.959\n",
      " 70%  |  100.0%  |   46  | 1.000\n",
      " 71%  |  100.0%  |   53  | 1.000\n",
      " 72%  |  100.0%  |   60  | 1.000\n",
      " 73%  |  100.0%  |   62  | 1.000\n",
      " 74%  |  100.0%  |   74  | 1.000\n",
      " 75%  |  100.0%  |   70  | 1.000\n",
      " 76%  |  100.0%  |   66  | 1.000\n",
      " 77%  |  100.0%  |   63  | 1.000\n",
      " 78%  |  100.0%  |   63  | 1.000\n",
      " 79%  |  100.0%  |   67  | 1.000\n",
      " 80%  |  100.0%  |   64  | 1.000\n",
      " 81%  |  100.0%  |   66  | 1.000\n",
      " 82%  |  100.0%  |   60  | 1.000\n",
      " 83%  |  100.0%  |   63  | 1.000\n",
      " 84%  |  100.0%  |   57  | 1.000\n",
      " 85%  |  100.0%  |   61  | 1.000\n",
      " 86%  |  100.0%  |   60  | 1.000\n",
      " 87%  |  100.0%  |   66  | 1.000\n",
      " 88%  |  100.0%  |   62  | 1.000\n",
      " 89%  |   96.9%  |   64  | 0.999\n",
      " 90%  |  100.0%  |   60  | 1.000\n",
      " 91%  |  100.0%  |   63  | 1.000\n",
      " 92%  |  100.0%  |   58  | 1.000\n",
      " 93%  |  100.0%  |   58  | 1.000\n",
      " 94%  |  100.0%  |   59  | 1.000\n",
      " 95%  |  100.0%  |   44  | 1.000\n",
      " 96%  |  100.0%  |   49  | 1.000\n",
      " 97%  |   96.2%  |   53  | 0.999\n",
      " 98%  |  100.0%  |   42  | 1.000\n",
      " 99%  |  100.0%  |   49  | 1.000\n",
      "\n",
      "--- Worst Performing Percentages (Top 10) ---\n",
      "Label | Accuracy | Sample Count\n",
      "------------------------------\n",
      " 69%  |   95.3%  |   43\n",
      " 97%  |   96.2%  |   53\n",
      " 89%  |   96.9%  |   64\n",
      "  5%  |   97.3%  |   75\n",
      "  0%  |  100.0%  |  173\n",
      " 10%  |  100.0%  |   78\n",
      " 11%  |  100.0%  |   73\n",
      " 12%  |  100.0%  |   77\n",
      " 13%  |  100.0%  |   69\n",
      " 14%  |  100.0%  |   72\n",
      "\n",
      "--- Confusion Examples (First 10) ---\n",
      "True | Pred | Confidence | Filename\n",
      "--------------------------------------------------\n",
      "  5% |  55% |    0.996 | 5pct_22.png\n",
      "  5% |  55% |    0.996 | 5pct_23.png\n",
      " 97% |  57% |    0.990 | 97pct_4.png\n",
      " 97% |  57% |    0.975 | 97pct_1.png\n",
      " 69% |  59% |    0.967 | 69pct_16.png\n",
      " 69% |  59% |    0.967 | 69pct_25.png\n",
      " 89% |  69% |    0.965 | 89pct_12.png\n",
      " 89% |  69% |    0.963 | 89pct_1.png\n",
      "\n",
      "--- Evaluation Summary ---\n",
      "Model performs at 99.88% accuracy on the original dataset\n",
      "Total misclassifications: 8\n",
      "High-confidence errors (>80%): 8\n",
      "\n",
      "Evaluation complete!\n",
      "\n",
      "--- Overall Performance ---\n",
      "Total Images: 6543\n",
      "Correct Predictions: 6535\n",
      "Overall Accuracy: 99.88%\n",
      "Average Loss: 0.0073\n",
      "Error Rate: 0.12%\n",
      "\n",
      "--- Per-Percentage Performance ---\n",
      "Label | Accuracy | Count | Avg Confidence\n",
      "---------------------------------------------\n",
      "  0%  |  100.0%  |  173  | 1.000\n",
      "  1%  |  100.0%  |   81  | 1.000\n",
      "  2%  |  100.0%  |   76  | 1.000\n",
      "  3%  |  100.0%  |   71  | 1.000\n",
      "  4%  |  100.0%  |   71  | 1.000\n",
      "  5%  |   97.3%  |   75  | 1.000\n",
      "  6%  |  100.0%  |   76  | 1.000\n",
      "  7%  |  100.0%  |   86  | 1.000\n",
      "  8%  |  100.0%  |   76  | 1.000\n",
      "  9%  |  100.0%  |   72  | 1.000\n",
      " 10%  |  100.0%  |   78  | 1.000\n",
      " 11%  |  100.0%  |   73  | 1.000\n",
      " 12%  |  100.0%  |   77  | 1.000\n",
      " 13%  |  100.0%  |   69  | 1.000\n",
      " 14%  |  100.0%  |   72  | 1.000\n",
      " 15%  |  100.0%  |   64  | 1.000\n",
      " 16%  |  100.0%  |   64  | 1.000\n",
      " 17%  |  100.0%  |   61  | 1.000\n",
      " 18%  |  100.0%  |   56  | 1.000\n",
      " 19%  |  100.0%  |   64  | 1.000\n",
      " 20%  |  100.0%  |   68  | 1.000\n",
      " 21%  |  100.0%  |   71  | 1.000\n",
      " 22%  |  100.0%  |   80  | 1.000\n",
      " 23%  |  100.0%  |   71  | 1.000\n",
      " 24%  |  100.0%  |   68  | 1.000\n",
      " 25%  |  100.0%  |   66  | 1.000\n",
      " 26%  |  100.0%  |   59  | 1.000\n",
      " 27%  |  100.0%  |   62  | 1.000\n",
      " 28%  |  100.0%  |   59  | 1.000\n",
      " 29%  |  100.0%  |   56  | 1.000\n",
      " 30%  |  100.0%  |   57  | 1.000\n",
      " 31%  |  100.0%  |   75  | 1.000\n",
      " 32%  |  100.0%  |   53  | 1.000\n",
      " 33%  |  100.0%  |   61  | 1.000\n",
      " 34%  |  100.0%  |   57  | 1.000\n",
      " 35%  |  100.0%  |   62  | 1.000\n",
      " 36%  |  100.0%  |   63  | 1.000\n",
      " 37%  |  100.0%  |   69  | 1.000\n",
      " 38%  |  100.0%  |   67  | 1.000\n",
      " 39%  |  100.0%  |   72  | 1.000\n",
      " 40%  |  100.0%  |   86  | 1.000\n",
      " 41%  |  100.0%  |   71  | 1.000\n",
      " 42%  |  100.0%  |   73  | 1.000\n",
      " 43%  |  100.0%  |   51  | 1.000\n",
      " 44%  |  100.0%  |   59  | 1.000\n",
      " 45%  |  100.0%  |   54  | 1.000\n",
      " 46%  |  100.0%  |   60  | 1.000\n",
      " 47%  |  100.0%  |   59  | 1.000\n",
      " 48%  |  100.0%  |   63  | 1.000\n",
      " 49%  |  100.0%  |   63  | 1.000\n",
      " 50%  |  100.0%  |   66  | 1.000\n",
      " 51%  |  100.0%  |   71  | 1.000\n",
      " 52%  |  100.0%  |   72  | 1.000\n",
      " 53%  |  100.0%  |   54  | 1.000\n",
      " 54%  |  100.0%  |   72  | 1.000\n",
      " 55%  |  100.0%  |   50  | 0.999\n",
      " 56%  |  100.0%  |   61  | 1.000\n",
      " 57%  |  100.0%  |   50  | 1.000\n",
      " 58%  |  100.0%  |   67  | 1.000\n",
      " 59%  |  100.0%  |   66  | 0.997\n",
      " 60%  |  100.0%  |   64  | 1.000\n",
      " 61%  |  100.0%  |   60  | 1.000\n",
      " 62%  |  100.0%  |   64  | 1.000\n",
      " 63%  |  100.0%  |  115  | 1.000\n",
      " 64%  |  100.0%  |   60  | 1.000\n",
      " 65%  |  100.0%  |   59  | 1.000\n",
      " 66%  |  100.0%  |   62  | 1.000\n",
      " 67%  |  100.0%  |   63  | 1.000\n",
      " 68%  |  100.0%  |   72  | 1.000\n",
      " 69%  |   95.3%  |   43  | 0.959\n",
      " 70%  |  100.0%  |   46  | 1.000\n",
      " 71%  |  100.0%  |   53  | 1.000\n",
      " 72%  |  100.0%  |   60  | 1.000\n",
      " 73%  |  100.0%  |   62  | 1.000\n",
      " 74%  |  100.0%  |   74  | 1.000\n",
      " 75%  |  100.0%  |   70  | 1.000\n",
      " 76%  |  100.0%  |   66  | 1.000\n",
      " 77%  |  100.0%  |   63  | 1.000\n",
      " 78%  |  100.0%  |   63  | 1.000\n",
      " 79%  |  100.0%  |   67  | 1.000\n",
      " 80%  |  100.0%  |   64  | 1.000\n",
      " 81%  |  100.0%  |   66  | 1.000\n",
      " 82%  |  100.0%  |   60  | 1.000\n",
      " 83%  |  100.0%  |   63  | 1.000\n",
      " 84%  |  100.0%  |   57  | 1.000\n",
      " 85%  |  100.0%  |   61  | 1.000\n",
      " 86%  |  100.0%  |   60  | 1.000\n",
      " 87%  |  100.0%  |   66  | 1.000\n",
      " 88%  |  100.0%  |   62  | 1.000\n",
      " 89%  |   96.9%  |   64  | 0.999\n",
      " 90%  |  100.0%  |   60  | 1.000\n",
      " 91%  |  100.0%  |   63  | 1.000\n",
      " 92%  |  100.0%  |   58  | 1.000\n",
      " 93%  |  100.0%  |   58  | 1.000\n",
      " 94%  |  100.0%  |   59  | 1.000\n",
      " 95%  |  100.0%  |   44  | 1.000\n",
      " 96%  |  100.0%  |   49  | 1.000\n",
      " 97%  |   96.2%  |   53  | 0.999\n",
      " 98%  |  100.0%  |   42  | 1.000\n",
      " 99%  |  100.0%  |   49  | 1.000\n",
      "\n",
      "--- Worst Performing Percentages (Top 10) ---\n",
      "Label | Accuracy | Sample Count\n",
      "------------------------------\n",
      " 69%  |   95.3%  |   43\n",
      " 97%  |   96.2%  |   53\n",
      " 89%  |   96.9%  |   64\n",
      "  5%  |   97.3%  |   75\n",
      "  0%  |  100.0%  |  173\n",
      " 10%  |  100.0%  |   78\n",
      " 11%  |  100.0%  |   73\n",
      " 12%  |  100.0%  |   77\n",
      " 13%  |  100.0%  |   69\n",
      " 14%  |  100.0%  |   72\n",
      "\n",
      "--- Confusion Examples (First 10) ---\n",
      "True | Pred | Confidence | Filename\n",
      "--------------------------------------------------\n",
      "  5% |  55% |    0.996 | 5pct_22.png\n",
      "  5% |  55% |    0.996 | 5pct_23.png\n",
      " 97% |  57% |    0.990 | 97pct_4.png\n",
      " 97% |  57% |    0.975 | 97pct_1.png\n",
      " 69% |  59% |    0.967 | 69pct_16.png\n",
      " 69% |  59% |    0.967 | 69pct_25.png\n",
      " 89% |  69% |    0.965 | 89pct_12.png\n",
      " 89% |  69% |    0.963 | 89pct_1.png\n",
      "\n",
      "--- Evaluation Summary ---\n",
      "Model performs at 99.88% accuracy on the original dataset\n",
      "Total misclassifications: 8\n",
      "High-confidence errors (>80%): 8\n",
      "\n",
      "Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluation on Original Dataset (dataset_merged) ---\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATING MODEL ON ORIGINAL DATASET (dataset_merged)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the original dataset for evaluation\n",
    "eval_dataset = PercentageDataset(img_dir='dataset_merged', transform=data_transforms)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Original dataset size: {len(eval_dataset)} images\")\n",
    "\n",
    "# Evaluation metrics\n",
    "model.eval()\n",
    "eval_correct = 0\n",
    "eval_total = 0\n",
    "eval_loss = 0.0\n",
    "predictions_by_label = {}\n",
    "confusion_data = []\n",
    "\n",
    "# Track performance per percentage\n",
    "percentage_stats = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(eval_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        eval_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        confidences = torch.softmax(outputs, 1)\n",
    "        \n",
    "        # Process each item in the batch\n",
    "        for i in range(len(images)):\n",
    "            true_label = labels[i].item()\n",
    "            pred_label = predicted[i].item()\n",
    "            confidence = confidences[i][pred_label].item()\n",
    "            \n",
    "            eval_total += 1\n",
    "            \n",
    "            # Initialize percentage stats if needed\n",
    "            if true_label not in percentage_stats:\n",
    "                percentage_stats[true_label] = {'correct': 0, 'total': 0, 'confidences': []}\n",
    "            \n",
    "            percentage_stats[true_label]['total'] += 1\n",
    "            percentage_stats[true_label]['confidences'].append(confidence)\n",
    "            \n",
    "            if pred_label == true_label:\n",
    "                eval_correct += 1\n",
    "                percentage_stats[true_label]['correct'] += 1\n",
    "            else:\n",
    "                # Store confusion data\n",
    "                img_idx = batch_idx * eval_loader.batch_size + i\n",
    "                filename = eval_dataset.image_files[img_idx]\n",
    "                confusion_data.append({\n",
    "                    'filename': filename,\n",
    "                    'true': true_label,\n",
    "                    'pred': pred_label,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "\n",
    "eval_loss /= len(eval_loader)\n",
    "eval_accuracy = 100 * eval_correct / eval_total\n",
    "\n",
    "print(f\"\\n--- Overall Performance ---\")\n",
    "print(f\"Total Images: {eval_total}\")\n",
    "print(f\"Correct Predictions: {eval_correct}\")\n",
    "print(f\"Overall Accuracy: {eval_accuracy:.2f}%\")\n",
    "print(f\"Average Loss: {eval_loss:.4f}\")\n",
    "print(f\"Error Rate: {100 - eval_accuracy:.2f}%\")\n",
    "\n",
    "# Show per-percentage performance\n",
    "print(f\"\\n--- Per-Percentage Performance ---\")\n",
    "print(\"Label | Accuracy | Count | Avg Confidence\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "sorted_percentages = sorted(percentage_stats.keys())\n",
    "for pct in sorted_percentages:\n",
    "    stats = percentage_stats[pct]\n",
    "    accuracy = (stats['correct'] / stats['total']) * 100 if stats['total'] > 0 else 0\n",
    "    avg_conf = sum(stats['confidences']) / len(stats['confidences']) if stats['confidences'] else 0\n",
    "    print(f\"{pct:3d}%  | {accuracy:6.1f}%  | {stats['total']:4d}  | {avg_conf:.3f}\")\n",
    "\n",
    "# Show worst performing percentages\n",
    "print(f\"\\n--- Worst Performing Percentages (Top 10) ---\")\n",
    "worst_performers = []\n",
    "for pct, stats in percentage_stats.items():\n",
    "    if stats['total'] >= 5:  # Only consider percentages with at least 5 samples\n",
    "        accuracy = (stats['correct'] / stats['total']) * 100\n",
    "        worst_performers.append((pct, accuracy, stats['total']))\n",
    "\n",
    "worst_performers.sort(key=lambda x: x[1])  # Sort by accuracy\n",
    "print(\"Label | Accuracy | Sample Count\")\n",
    "print(\"-\" * 30)\n",
    "for pct, acc, count in worst_performers[:10]:\n",
    "    print(f\"{pct:3d}%  | {acc:6.1f}%  | {count:4d}\")\n",
    "\n",
    "# Show confusion examples\n",
    "if confusion_data:\n",
    "    print(f\"\\n--- Confusion Examples (First 10) ---\")\n",
    "    print(\"True | Pred | Confidence | Filename\")\n",
    "    print(\"-\" * 50)\n",
    "    for conf_item in sorted(confusion_data, key=lambda x: x['confidence'], reverse=True)[:10]:\n",
    "        print(f\"{conf_item['true']:3d}% | {conf_item['pred']:3d}% | {conf_item['confidence']:8.3f} | {conf_item['filename']}\")\n",
    "\n",
    "print(f\"\\n--- Evaluation Summary ---\")\n",
    "print(f\"Model performs at {eval_accuracy:.2f}% accuracy on the original dataset\")\n",
    "print(f\"Total misclassifications: {len(confusion_data)}\")\n",
    "if confusion_data:\n",
    "    high_conf_errors = [c for c in confusion_data if c['confidence'] > 0.8]\n",
    "    print(f\"High-confidence errors (>80%): {len(high_conf_errors)}\")\n",
    "    \n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775fb3a0",
   "metadata": {},
   "source": [
    "# Create Bias Training Dataset\n",
    "\n",
    "This section creates a new dataset called `dataset_final` that combines:\n",
    "1. All original images from `dataset_merged`\n",
    "2. Failed validation samples from `dataset_failed` (1x ratio)\n",
    "3. Shift-transformed versions of failed samples (0.7x ratio)\n",
    "4. Other transformations of failed samples (0.3x ratio - scaling, diagonal, rotation)\n",
    "\n",
    "This biased dataset will help the model learn better on the samples it previously struggled with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b720a6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created final dataset directory: dataset_final\n",
      "\n",
      "--- Copying Amplified Dataset ---\n",
      "Copied 16360 amplified files to dataset_final\n",
      "\n",
      "--- Processing Failed Validation Files from Original Dataset ---\n",
      "Found 2517 failed validation files\n",
      "Processing 2176 unique original files from failed validation samples...\n",
      "These correspond to 2517 failed validation samples (including transforms)\n",
      "\n",
      "Failed sample mapping summary:\n",
      "  0pct_101.png -> 1 failed variants: 0pct_101_scale1.png\n",
      "  0pct_112.png -> 1 failed variants: 0pct_112_trans1.png\n",
      "  0pct_121.png -> 1 failed variants: 0pct_121_trans1.png\n",
      "  0pct_122.png -> 1 failed variants: 0pct_122_trans1.png\n",
      "  0pct_136.png -> 1 failed variants: 0pct_136_trans1.png\n",
      "  ... and 2171 more original files\n",
      "Copied 16360 amplified files to dataset_final\n",
      "\n",
      "--- Processing Failed Validation Files from Original Dataset ---\n",
      "Found 2517 failed validation files\n",
      "Processing 2176 unique original files from failed validation samples...\n",
      "These correspond to 2517 failed validation samples (including transforms)\n",
      "\n",
      "Failed sample mapping summary:\n",
      "  0pct_101.png -> 1 failed variants: 0pct_101_scale1.png\n",
      "  0pct_112.png -> 1 failed variants: 0pct_112_trans1.png\n",
      "  0pct_121.png -> 1 failed variants: 0pct_121_trans1.png\n",
      "  0pct_122.png -> 1 failed variants: 0pct_122_trans1.png\n",
      "  0pct_136.png -> 1 failed variants: 0pct_136_trans1.png\n",
      "  ... and 2171 more original files\n",
      "\n",
      "--- Bias Dataset Creation Complete ---\n",
      "Total failed samples added: 5010\n",
      "  - Original failed samples: 2176\n",
      "  - Translation transforms: 2176\n",
      "  - Other transforms: 658\n",
      "\n",
      "Final dataset 'dataset_final' contains 21370 total images\n",
      "Amplified dataset had 16360 images\n",
      "Added 5010 bias training samples from failed originals\n",
      "\n",
      "Final dataset contains 100 unique labels\n",
      "Label distribution (top 10 most common):\n",
      "  0%: 476 images\n",
      "  63%: 430 images\n",
      "  22%: 299 images\n",
      "  51%: 270 images\n",
      "  10%: 269 images\n",
      "  12%: 264 images\n",
      "  6%: 264 images\n",
      "  74%: 257 images\n",
      "  42%: 255 images\n",
      "  68%: 254 images\n",
      "\n",
      "ðŸŽ¯ Dataset Composition:\n",
      "   â€¢ Amplified dataset: 16360 images\n",
      "   â€¢ Failed validation samples: 2517 (including transforms)\n",
      "   â€¢ Unique original failed files: 2176 files\n",
      "   â€¢ Total bias samples added: 5010\n",
      "   â€¢ Final combined dataset: 21370 images\n",
      "\n",
      "ðŸŽ¯ To use this final dataset for training, ensure the img_dir parameter above is set to:\n",
      "   full_dataset = PercentageDataset(img_dir='dataset_final', transform=data_transforms)\n",
      "\n",
      "--- Bias Dataset Creation Complete ---\n",
      "Total failed samples added: 5010\n",
      "  - Original failed samples: 2176\n",
      "  - Translation transforms: 2176\n",
      "  - Other transforms: 658\n",
      "\n",
      "Final dataset 'dataset_final' contains 21370 total images\n",
      "Amplified dataset had 16360 images\n",
      "Added 5010 bias training samples from failed originals\n",
      "\n",
      "Final dataset contains 100 unique labels\n",
      "Label distribution (top 10 most common):\n",
      "  0%: 476 images\n",
      "  63%: 430 images\n",
      "  22%: 299 images\n",
      "  51%: 270 images\n",
      "  10%: 269 images\n",
      "  12%: 264 images\n",
      "  6%: 264 images\n",
      "  74%: 257 images\n",
      "  42%: 255 images\n",
      "  68%: 254 images\n",
      "\n",
      "ðŸŽ¯ Dataset Composition:\n",
      "   â€¢ Amplified dataset: 16360 images\n",
      "   â€¢ Failed validation samples: 2517 (including transforms)\n",
      "   â€¢ Unique original failed files: 2176 files\n",
      "   â€¢ Total bias samples added: 5010\n",
      "   â€¢ Final combined dataset: 21370 images\n",
      "\n",
      "ðŸŽ¯ To use this final dataset for training, ensure the img_dir parameter above is set to:\n",
      "   full_dataset = PercentageDataset(img_dir='dataset_final', transform=data_transforms)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# --- Configuration ---\n",
    "AMPLIFIED_DATASET = \"dataset_amplified\"  # Source of amplified data\n",
    "ORIGINAL_DATASET = \"dataset_merged\"      # Source for failed samples\n",
    "FAILED_DATASET = \"dataset_failed\"        # Failed validation files\n",
    "FINAL_DATASET = \"dataset_final\"          # Final combined dataset\n",
    "\n",
    "# Create the final dataset directory\n",
    "os.makedirs(FINAL_DATASET, exist_ok=True)\n",
    "print(f\"Created final dataset directory: {FINAL_DATASET}\")\n",
    "\n",
    "# --- Step 1: Copy all amplified images from dataset_amplified ---\n",
    "print(\"\\n--- Copying Amplified Dataset ---\")\n",
    "amplified_files = [f for f in os.listdir(AMPLIFIED_DATASET) if f.endswith('.png')]\n",
    "global_counter = Counter()\n",
    "\n",
    "# Initialize counter with existing amplified files\n",
    "for filename in amplified_files:\n",
    "    match = re.match(r'^(\\d+)pct_(\\d+)\\.png$', filename)\n",
    "    if match:\n",
    "        label = match.group(1)\n",
    "        counter = int(match.group(2))\n",
    "        global_counter[label] = max(global_counter[label], counter)\n",
    "\n",
    "# Copy amplified files\n",
    "files_copied = 0\n",
    "for filename in amplified_files:\n",
    "    src_path = os.path.join(AMPLIFIED_DATASET, filename)\n",
    "    dst_path = os.path.join(FINAL_DATASET, filename)\n",
    "    shutil.copy(src_path, dst_path)\n",
    "    files_copied += 1\n",
    "\n",
    "print(f\"Copied {files_copied} amplified files to {FINAL_DATASET}\")\n",
    "\n",
    "# --- Step 2: Process failed validation files from original dataset ---\n",
    "print(\"\\n--- Processing Failed Validation Files from Original Dataset ---\")\n",
    "failed_files = [f for f in os.listdir(FAILED_DATASET) if f.endswith('.png')]\n",
    "print(f\"Found {len(failed_files)} failed validation files\")\n",
    "\n",
    "# For each failed file, find the corresponding original file in dataset_merged\n",
    "failed_originals = []\n",
    "failed_originals_mapping = {}  # Map from original filename to failed filename for reference\n",
    "\n",
    "for failed_filename in failed_files:\n",
    "    # Extract the base filename by removing transformation suffixes\n",
    "    # Pattern: {label}pct_{number}_{transform}.png -> {label}pct_{base_number}.png\n",
    "    match = re.match(r'^(\\d+)pct_(\\d+)(?:_(?:trans|diag|scale)\\d*)?\\.png$', failed_filename)\n",
    "    \n",
    "    if match:\n",
    "        label = match.group(1)\n",
    "        base_number = match.group(2)\n",
    "        \n",
    "        # Try to find the original file in dataset_merged\n",
    "        # Look for files with the same label and base number\n",
    "        original_pattern = f\"{label}pct_{base_number}.png\"\n",
    "        original_path = os.path.join(ORIGINAL_DATASET, original_pattern)\n",
    "        \n",
    "        if os.path.exists(original_path):\n",
    "            if original_pattern not in failed_originals_mapping:\n",
    "                failed_originals.append(original_pattern)\n",
    "                failed_originals_mapping[original_pattern] = []\n",
    "            failed_originals_mapping[original_pattern].append(failed_filename)\n",
    "        else:\n",
    "            # Try to find any file with the same label and a similar base number\n",
    "            # Sometimes the numbering might be slightly different\n",
    "            found_alternative = False\n",
    "            for alt_file in os.listdir(ORIGINAL_DATASET):\n",
    "                if alt_file.startswith(f\"{label}pct_\") and alt_file.endswith('.png'):\n",
    "                    alt_match = re.match(rf'^{label}pct_(\\d+)\\.png$', alt_file)\n",
    "                    if alt_match:\n",
    "                        alt_number = int(alt_match.group(1))\n",
    "                        base_num = int(base_number)\n",
    "                        # Allow for some variance in numbering (within 5)\n",
    "                        if abs(alt_number - base_num) <= 5:\n",
    "                            if alt_file not in failed_originals_mapping:\n",
    "                                failed_originals.append(alt_file)\n",
    "                                failed_originals_mapping[alt_file] = []\n",
    "                            failed_originals_mapping[alt_file].append(failed_filename)\n",
    "                            found_alternative = True\n",
    "                            break\n",
    "            \n",
    "            if not found_alternative:\n",
    "                print(f\"Warning: No original file found for failed sample {failed_filename}\")\n",
    "    else:\n",
    "        print(f\"Warning: Could not parse failed filename {failed_filename}\")\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "failed_originals = list(dict.fromkeys(failed_originals))\n",
    "\n",
    "# Define transformation functions\n",
    "def apply_translation(image, max_shift=8):\n",
    "    \"\"\"Apply random translation/shift transformation\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    dx = np.random.randint(-max_shift, max_shift + 1)\n",
    "    dy = np.random.randint(-max_shift, max_shift + 1)\n",
    "    \n",
    "    M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "    transformed = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "    return transformed\n",
    "\n",
    "def apply_scaling(image, scale_range=(0.85, 1.15)):\n",
    "    \"\"\"Apply random scaling transformation\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    scale = np.random.uniform(*scale_range)\n",
    "    \n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, 0, scale)\n",
    "    transformed = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "    return transformed\n",
    "\n",
    "def apply_rotation(image, angle_range=(-15, 15)):\n",
    "    \"\"\"Apply random rotation transformation\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    angle = np.random.uniform(*angle_range)\n",
    "    \n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    transformed = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "    return transformed\n",
    "\n",
    "def apply_diagonal_shift(image, max_shift=6):\n",
    "    \"\"\"Apply diagonal transformation\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    dx = np.random.randint(-max_shift, max_shift + 1)\n",
    "    dy = np.random.randint(-max_shift, max_shift + 1)\n",
    "    \n",
    "    # Ensure diagonal movement (both dx and dy are non-zero)\n",
    "    if dx == 0:\n",
    "        dx = np.random.choice([-max_shift//2, max_shift//2])\n",
    "    if dy == 0:\n",
    "        dy = np.random.choice([-max_shift//2, max_shift//2])\n",
    "    \n",
    "    M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "    transformed = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "    return transformed\n",
    "\n",
    "# Process each failed file from the original dataset\n",
    "total_failed_added = 0\n",
    "transform_stats = {'original': 0, 'translation': 0, 'other': 0}\n",
    "\n",
    "print(f\"Processing {len(failed_originals)} unique original files from failed validation samples...\")\n",
    "print(f\"These correspond to {len(failed_files)} failed validation samples (including transforms)\")\n",
    "\n",
    "# Show mapping summary\n",
    "print(f\"\\nFailed sample mapping summary:\")\n",
    "for original_file, failed_variants in list(failed_originals_mapping.items())[:5]:  # Show first 5\n",
    "    print(f\"  {original_file} -> {len(failed_variants)} failed variants: {', '.join(failed_variants[:3])}{'...' if len(failed_variants) > 3 else ''}\")\n",
    "if len(failed_originals_mapping) > 5:\n",
    "    print(f\"  ... and {len(failed_originals_mapping) - 5} more original files\")\n",
    "\n",
    "for filename in failed_originals:\n",
    "    # Extract label from filename\n",
    "    match = re.match(r'^(\\d+)', filename)\n",
    "    if not match:\n",
    "        print(f\"Warning: Could not extract label from {filename}, skipping\")\n",
    "        continue\n",
    "    \n",
    "    label = match.group(1)\n",
    "    \n",
    "    # Load the failed image from the ORIGINAL dataset (not the failed folder)\n",
    "    src_path = os.path.join(ORIGINAL_DATASET, filename)\n",
    "    image = cv2.imread(src_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if image is None:\n",
    "        print(f\"Warning: Could not load image {filename}, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # 1. Add original failed file (1x ratio)\n",
    "    global_counter[label] += 1\n",
    "    new_filename = f\"{label}pct_{global_counter[label]}.png\"\n",
    "    dst_path = os.path.join(FINAL_DATASET, new_filename)\n",
    "    cv2.imwrite(dst_path, image)\n",
    "    total_failed_added += 1\n",
    "    transform_stats['original'] += 1\n",
    "    \n",
    "    # 2. Add translation-transformed versions (0.7x ratio)\n",
    "    # Generate 1 translation for every failed image (0.7x â‰ˆ 1x for simplicity)\n",
    "    global_counter[label] += 1\n",
    "    transformed_img = apply_translation(image)\n",
    "    trans_filename = f\"{label}pct_{global_counter[label]}.png\"\n",
    "    trans_path = os.path.join(FINAL_DATASET, trans_filename)\n",
    "    cv2.imwrite(trans_path, transformed_img)\n",
    "    total_failed_added += 1\n",
    "    transform_stats['translation'] += 1\n",
    "    \n",
    "    # 3. Add other transformations (0.3x ratio)\n",
    "    # Randomly choose between scaling, rotation, or diagonal shift\n",
    "    # For every 3 failed images, add 1 other transform (roughly 0.3x)\n",
    "    if np.random.random() < 0.3:\n",
    "        transform_type = np.random.choice(['scale', 'rotation', 'diagonal'])\n",
    "        \n",
    "        if transform_type == 'scale':\n",
    "            other_transformed = apply_scaling(image)\n",
    "        elif transform_type == 'rotation':\n",
    "            other_transformed = apply_rotation(image)\n",
    "        else:  # diagonal\n",
    "            other_transformed = apply_diagonal_shift(image)\n",
    "        \n",
    "        global_counter[label] += 1\n",
    "        other_filename = f\"{label}pct_{global_counter[label]}.png\"\n",
    "        other_path = os.path.join(FINAL_DATASET, other_filename)\n",
    "        cv2.imwrite(other_path, other_transformed)\n",
    "        total_failed_added += 1\n",
    "        transform_stats['other'] += 1\n",
    "\n",
    "print(f\"\\n--- Bias Dataset Creation Complete ---\")\n",
    "print(f\"Total failed samples added: {total_failed_added}\")\n",
    "print(f\"  - Original failed samples: {transform_stats['original']}\")\n",
    "print(f\"  - Translation transforms: {transform_stats['translation']}\")\n",
    "print(f\"  - Other transforms: {transform_stats['other']}\")\n",
    "\n",
    "# Final dataset statistics\n",
    "final_files = [f for f in os.listdir(FINAL_DATASET) if f.endswith('.png')]\n",
    "print(f\"\\nFinal dataset '{FINAL_DATASET}' contains {len(final_files)} total images\")\n",
    "print(f\"Amplified dataset had {len(amplified_files)} images\")\n",
    "print(f\"Added {len(final_files) - len(amplified_files)} bias training samples from failed originals\")\n",
    "\n",
    "# Show label distribution in final dataset\n",
    "final_counter = Counter()\n",
    "for filename in final_files:\n",
    "    match = re.match(r'^(\\d+)', filename)\n",
    "    if match:\n",
    "        label = int(match.group(1))\n",
    "        final_counter[label] += 1\n",
    "\n",
    "print(f\"\\nFinal dataset contains {len(final_counter)} unique labels\")\n",
    "print(\"Label distribution (top 10 most common):\")\n",
    "for label, count in final_counter.most_common(10):\n",
    "    print(f\"  {label}%: {count} images\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Dataset Composition:\")\n",
    "print(f\"   â€¢ Amplified dataset: {len(amplified_files)} images\")\n",
    "print(f\"   â€¢ Failed validation samples: {len(failed_files)} (including transforms)\")\n",
    "print(f\"   â€¢ Unique original failed files: {len(failed_originals)} files\")\n",
    "print(f\"   â€¢ Total bias samples added: {total_failed_added}\")\n",
    "print(f\"   â€¢ Final combined dataset: {len(final_files)} images\")\n",
    "print(f\"\\nðŸŽ¯ To use this final dataset for training, ensure the img_dir parameter above is set to:\")\n",
    "print(f\"   full_dataset = PercentageDataset(img_dir='{FINAL_DATASET}', transform=data_transforms)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
